{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNti7Ui8oiiimqYr8EabxLc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/trendinafrica/Comp_Neuro-ML_course/blob/main/Fixed_Points_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "w6YzInaZXa8b"
      },
      "outputs": [],
      "source": [
        "### Installing and importing relevant packages"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ! git clone https://github.com/mattgolub/fixed-point-finder.git\n",
        "! git clone https://github.com/mattgolub/recurrent-whisperer.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t2IXZvLfwSeZ",
        "outputId": "7d75aa09-1547-417a-9fde-afdaaaa8cc23"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'recurrent-whisperer' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone -b dependabot/pip/tensorflow-2.11.1 https://github.com/mattgolub/fixed-point-finder.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mS-7jroW8G5A",
        "outputId": "d550a4cb-b73d-408f-d489-1de401ebedbf"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'fixed-point-finder' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install -r /content/fixed-point-finder/requirements-cpu.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qH8yN6V3xU-J",
        "outputId": "d67635ec-3dd2-486d-a4b7-bb22ddb742db"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Obtaining recurrent-whisperer from git+https://github.com/mattgolub/recurrent-whisperer.git@master#egg=recurrent-whisperer (from -r /content/fixed-point-finder/requirements-cpu.txt (line 7))\n",
            "  Updating ./src/recurrent-whisperer clone (to revision master)\n",
            "  Running command git fetch -q --tags\n",
            "  Running command git reset --hard -q 4e0a0ab8e3655174132a96b910044bf867fb1a81\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting numpy==1.14.5 (from -r /content/fixed-point-finder/requirements-cpu.txt (line 1))\n",
            "  Using cached numpy-1.14.5.zip (4.9 MB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting scipy==1.1.0 (from -r /content/fixed-point-finder/requirements-cpu.txt (line 2))\n",
            "  Using cached scipy-1.1.0.tar.gz (15.6 MB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting scikit-learn==0.20.0 (from -r /content/fixed-point-finder/requirements-cpu.txt (line 3))\n",
            "  Using cached scikit-learn-0.20.0.tar.gz (28.1 MB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting matplotlib==2.2.3 (from -r /content/fixed-point-finder/requirements-cpu.txt (line 4))\n",
            "  Using cached matplotlib-2.2.3.tar.gz (36.8 MB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting PyYAML==3.13 (from -r /content/fixed-point-finder/requirements-cpu.txt (line 5))\n",
            "  Using cached PyYAML-3.13.tar.gz (270 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting tensorflow==2.11.1 (from -r /content/fixed-point-finder/requirements-cpu.txt (line 6))\n",
            "  Using cached tensorflow-2.11.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (588.3 MB)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib==2.2.3->-r /content/fixed-point-finder/requirements-cpu.txt (line 4)) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib==2.2.3->-r /content/fixed-point-finder/requirements-cpu.txt (line 4)) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib==2.2.3->-r /content/fixed-point-finder/requirements-cpu.txt (line 4)) (2.8.2)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from matplotlib==2.2.3->-r /content/fixed-point-finder/requirements-cpu.txt (line 4)) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib==2.2.3->-r /content/fixed-point-finder/requirements-cpu.txt (line 4)) (1.16.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib==2.2.3->-r /content/fixed-point-finder/requirements-cpu.txt (line 4)) (1.4.4)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.1->-r /content/fixed-point-finder/requirements-cpu.txt (line 6)) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.1->-r /content/fixed-point-finder/requirements-cpu.txt (line 6)) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.1->-r /content/fixed-point-finder/requirements-cpu.txt (line 6)) (23.3.3)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.1->-r /content/fixed-point-finder/requirements-cpu.txt (line 6)) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.1->-r /content/fixed-point-finder/requirements-cpu.txt (line 6)) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.1->-r /content/fixed-point-finder/requirements-cpu.txt (line 6)) (1.54.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.1->-r /content/fixed-point-finder/requirements-cpu.txt (line 6)) (3.8.0)\n",
            "Collecting keras<2.12,>=2.11.0 (from tensorflow==2.11.1->-r /content/fixed-point-finder/requirements-cpu.txt (line 6))\n",
            "  Using cached keras-2.11.0-py2.py3-none-any.whl (1.7 MB)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.1->-r /content/fixed-point-finder/requirements-cpu.txt (line 6)) (16.0.0)\n",
            "INFO: pip is looking at multiple versions of tensorflow to determine which version is compatible with other requirements. This could take a while.\n",
            "\u001b[31mERROR: Cannot install -r /content/fixed-point-finder/requirements-cpu.txt (line 3), -r /content/fixed-point-finder/requirements-cpu.txt (line 4), -r /content/fixed-point-finder/requirements-cpu.txt (line 6) and numpy==1.14.5 because these package versions have conflicting dependencies.\u001b[0m\u001b[31m\n",
            "\u001b[0m\n",
            "The conflict is caused by:\n",
            "    The user requested numpy==1.14.5\n",
            "    scikit-learn 0.20.0 depends on numpy>=1.8.2\n",
            "    matplotlib 2.2.3 depends on numpy>=1.7.1\n",
            "    tensorflow 2.11.1 depends on numpy>=1.20\n",
            "\n",
            "To fix this you could try to:\n",
            "1. loosen the range of package versions you've specified\n",
            "2. remove package versions to allow pip attempt to solve the dependency conflict\n",
            "\n",
            "\u001b[31mERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/topics/dependency-resolution/#dealing-with-dependency-conflicts\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.insert(1, \"/content/recurrent-whisperer\")\n",
        "sys.path.insert(1, \"/content/fixed-point-finder\")"
      ],
      "metadata": {
        "id": "vPMWmtQZUmC6"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "print(sys.path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cWPC83I_agMU",
        "outputId": "c681e055-c183-424d-fbe8-6749ca8e0a6b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['/content', '/content/fixed-point-finder', '/content/recurrent-whisperer', '/env/python', '/usr/lib/python310.zip', '/usr/lib/python3.10', '/usr/lib/python3.10/lib-dynload', '', '/usr/local/lib/python3.10/dist-packages', '/usr/lib/python3/dist-packages', '/usr/local/lib/python3.10/dist-packages/IPython/extensions', '/root/.ipython']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install timer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "--nFrDtM-qTa",
        "outputId": "992d98c6-bb29-4607-c51b-6f494a4099ff"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: timer in /usr/local/lib/python3.10/dist-packages (0.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior() #added to enable v1 behavior LND\n",
        "tf.enable_eager_execution()\n",
        "# import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "import pdb\n",
        "import sys\n",
        "import argparse\n",
        "from tensorflow.keras.optimizers.legacy import Adam\n",
        "\n",
        "if os.environ.get('DISPLAY','') == '':\n",
        "    # Ensures smooth running across environments, including servers without\n",
        "    # graphical backends.\n",
        "    print('No display found. Using non-interactive Agg backend.')\n",
        "    import matplotlib\n",
        "    matplotlib.use('Agg')\n",
        "\n",
        "sys.path.insert(0, '/content/fixed-point-finder/')\n",
        "sys.path.insert(0, \"/content/recurrent-whisperer\")\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "# from FlipFlop import FlipFlop\n",
        "from RecurrentWhisperer import RecurrentWhisperer\n",
        "from FixedPointFinder import FixedPointFinder\n",
        "from FixedPoints import FixedPoints\n",
        "from plot_utils import plot_fps\n",
        "import tf_utils"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qniAGAVpaydA",
        "outputId": "9a92df11-be37-42e0-cd25-c2ddab80c2cf"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No display found. Using non-interactive Agg backend.\n",
            "No display found. Using non-interactive Agg backend.\n",
            "No display found. Using non-interactive Agg backend.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class FlipFlop(RecurrentWhisperer):\n",
        "    ''' Class for training an RNN to implement an N-bit memory, a.k.a. \"the\n",
        "    flip-flop  task\" as described in Sussillo & Barak, Neural Computation,\n",
        "    2013.\n",
        "\n",
        "    Task:\n",
        "        Briefly, a set of inputs carry transient pulses (-1 or +1) to set the\n",
        "        state of a set of binary outputs (also -1 or +1). Each input drives\n",
        "        exactly one output. If the sign of an input pulse opposes the sign\n",
        "        currently held at the corresponding output, the sign of the output\n",
        "        flips. If an input pulse's sign matches that currently held at the\n",
        "        corresponding output, the output does not change.\n",
        "\n",
        "        This class generates synthetic data for the flip-flop task via\n",
        "        generate_flipflop_trials(...).\n",
        "\n",
        "    Usage:\n",
        "        This class trains an RNN to generate the correct outputs given the\n",
        "        inputs of the flip-flop task. All that is needed to get started is to\n",
        "        construct a flipflop object and to call .train on that object:\n",
        "\n",
        "        # dict of hyperparameter key/value pairs\n",
        "        # (see 'Hyperparameters' section below)\n",
        "        hps = {...}\n",
        "\n",
        "        ff = FlipFlop(**hps)\n",
        "        ff.train()\n",
        "\n",
        "    Hyperparameters:\n",
        "        rnn_type: string specifying the architecture of the RNN. Currently\n",
        "        must be one of {'vanilla', 'gru', 'lstm'}. Default: 'vanilla'.\n",
        "\n",
        "        n_hidden: int specifying the number of hidden units in the RNN.\n",
        "        Default: 24.\n",
        "\n",
        "        data_hps: dict containing hyperparameters for generating synthetic\n",
        "        data. Contains the following keys:\n",
        "\n",
        "            'n_batch': int specifying the number of synthetic trials to use\n",
        "            per training batch (i.e., for one gradient step). Default: 128.\n",
        "\n",
        "            'n_time': int specifying the duration of each synthetic trial\n",
        "            (measured in timesteps). Default: 256.\n",
        "\n",
        "            'n_bits': int specifying the number of input channels into the\n",
        "            FlipFlop device (which will also be the number of output channels).\n",
        "            Default: 3.\n",
        "\n",
        "            'p_flip': float between 0.0 and 1.0 specifying the probability\n",
        "            that a particular input channel at a particular timestep will\n",
        "            contain a pulse (-1 or +1) on top of its steady-state value (0).\n",
        "            Pulse signs are chosen by fair coin flips, and pulses are produced\n",
        "            with the same statistics across all input channels and across all\n",
        "            timesteps (i.e., there are no history effects, there are no\n",
        "            interactions across input channels). Default: 0.2.\n",
        "\n",
        "        log_dir: string specifying the top-level directory for saving various\n",
        "        training runs (where each training run is specified by a different set\n",
        "        of hyperparameter settings). When tuning hyperparameters, log_dir is\n",
        "        meant to be constant across models. Default: '/tmp/flipflop_logs/'.\n",
        "\n",
        "        n_trials_plot: int specifying the number of synthetic trials to plot\n",
        "        per visualization update. Default: 4.\n",
        "    '''\n",
        "\n",
        "    @staticmethod\n",
        "    def _default_hash_hyperparameters():\n",
        "        '''Defines default hyperparameters, specific to FlipFlop, for the set\n",
        "        of hyperparameters that are hashed to define a directory structure for\n",
        "        easily managing multiple runs of the RNN training (i.e., using\n",
        "        different hyperparameter settings). Additional default hyperparameters\n",
        "        are defined in RecurrentWhisperer (from which FlipFlop inherits).\n",
        "\n",
        "        Args:\n",
        "            None.\n",
        "\n",
        "        Returns:\n",
        "            dict of hyperparameters.\n",
        "        '''\n",
        "        return {\n",
        "            'rnn_type': 'vanilla',\n",
        "            'n_hidden': 24,\n",
        "            'data_hps': {\n",
        "                'n_batch': 128,\n",
        "                'n_time': 256,\n",
        "                'n_bits': 3,\n",
        "                'p_flip': 0.2}\n",
        "            }\n",
        "\n",
        "    @staticmethod\n",
        "    def _default_non_hash_hyperparameters():\n",
        "        '''Defines default hyperparameters, specific to FlipFlop, for the set\n",
        "        of hyperparameters that are NOT hashed. Additional default\n",
        "        hyperparameters are defined in RecurrentWhisperer (from which FlipFlop\n",
        "        inherits).\n",
        "\n",
        "        Args:\n",
        "            None.\n",
        "\n",
        "        Returns:\n",
        "            dict of hyperparameters.\n",
        "        '''\n",
        "        return {\n",
        "            'log_dir': '/tmp/flipflop_logs/',\n",
        "            'n_trials_plot': 1,\n",
        "            }\n",
        "\n",
        "    def _setup_model(self):\n",
        "        '''Defines an RNN in Tensorflow.\n",
        "\n",
        "        See docstring in RecurrentWhisperer.\n",
        "\n",
        "        '''\n",
        "\n",
        "        hps = self.hps\n",
        "        n_hidden = hps.n_hidden\n",
        "\n",
        "        data_hps = hps.data_hps\n",
        "        n_batch = data_hps['n_batch']\n",
        "        n_time = data_hps['n_time']\n",
        "        n_inputs = data_hps['n_bits']\n",
        "        n_output = n_inputs\n",
        "\n",
        "        # Data handling\n",
        "        self.inputs_bxtxd = tf.placeholder(tf.float32,\n",
        "            [n_batch, n_time, n_inputs])\n",
        "        self.output_bxtxd = tf.placeholder(tf.float32,\n",
        "            [n_batch, n_time, n_output])\n",
        "\n",
        "        # RNN\n",
        "        if hps.rnn_type == 'vanilla':\n",
        "            self.rnn_cell = tf.nn.rnn_cell.BasicRNNCell(n_hidden)\n",
        "        elif hps.rnn_type == 'gru':\n",
        "            self.rnn_cell = tf.nn.rnn_cell.GRUCell(n_hidden)\n",
        "        elif hps.rnn_type == 'lstm':\n",
        "            self.rnn_cell = tf.nn.rnn_cell.LSTMCell(n_hidden)\n",
        "        else:\n",
        "            raise ValueError('Hyperparameter rnn_type must be one of '\n",
        "                '[vanilla, gru, lstm] but was %s' % hps.rnn_type)\n",
        "\n",
        "        initial_state = self.rnn_cell.zero_state(n_batch, dtype=tf.float32)\n",
        "\n",
        "        if hps.rnn_type == 'lstm':\n",
        "            self.state_bxtxd = tf_utils.unroll_LSTM(\n",
        "                self.rnn_cell,\n",
        "                inputs=self.inputs_bxtxd,\n",
        "                initial_state=initial_state)\n",
        "\n",
        "            self.hidden_bxtxd = self.state_bxtxd.h\n",
        "\n",
        "        else:\n",
        "            self.state_bxtxd, _ = tf.nn.dynamic_rnn(\n",
        "                self.rnn_cell,\n",
        "                inputs=self.inputs_bxtxd,\n",
        "                initial_state=initial_state)\n",
        "\n",
        "            self.hidden_bxtxd = self.state_bxtxd\n",
        "\n",
        "        # Readout from RNN\n",
        "        np_W_out, np_b_out = self._np_init_weight_matrix(n_hidden, n_output)\n",
        "        self.W_out = tf.Variable(np_W_out, dtype=tf.float32, name='W_out')\n",
        "        self.b_out = tf.Variable(np_b_out, dtype=tf.float32, name='b_out')\n",
        "        self.pred_output_bxtxd = tf.tensordot(self.hidden_bxtxd,\n",
        "            self.W_out, axes=1) + self.b_out\n",
        "\n",
        "        # Loss\n",
        "        self.loss = tf.reduce_mean(\n",
        "            tf.squared_difference(self.output_bxtxd, self.pred_output_bxtxd))\n",
        "\n",
        "    def _np_init_weight_matrix(self, input_size, output_size):\n",
        "        '''Randomly initializes a weight matrix W and bias vector b.\n",
        "\n",
        "        For use with input data matrix X [n x input_size] and output data\n",
        "        matrix Y [n x output_size], such that Y = X*W + b (with broadcast\n",
        "        addition). This is the typical required usage for TF dynamic_rnn.\n",
        "\n",
        "        Weights drawn from a standard normal distribution and are then\n",
        "        rescaled to preserve input-output variance.\n",
        "\n",
        "        Args:\n",
        "            input_size: non-negative int specifying the number of input\n",
        "            dimensions of the linear mapping.\n",
        "\n",
        "            output_size: non-negative int specifying the number of output\n",
        "            dimensions of the linear mapping.\n",
        "\n",
        "        Returns:\n",
        "            W: numpy array of shape [input_size x output_size] containing\n",
        "            randomly initialized weights.\n",
        "\n",
        "            b: numpy array of shape [output_size,] containing all zeros.\n",
        "        '''\n",
        "        if input_size == 0:\n",
        "            scale = 1.0 # This avoids divide by zero error\n",
        "        else:\n",
        "            scale = 1.0 / np.sqrt(input_size)\n",
        "        W = np.multiply(scale,self.rng.randn(input_size, output_size))\n",
        "        b = np.zeros(output_size)\n",
        "        return W, b\n",
        "\n",
        "    def _build_data_feed_dict(self, data, **kwargs):\n",
        "        '''Performs a training step over a single batch of data.\n",
        "\n",
        "        Args:\n",
        "            data: dict containing one epoch of data. Contains the\n",
        "            following key/value pairs:\n",
        "\n",
        "                'inputs': [n_batch x n_time x n_bits] numpy array specifying\n",
        "                the inputs to the RNN.\n",
        "\n",
        "                'outputs': [n_batch x n_time x n_bits] numpy array specifying\n",
        "                the correct output responses to the 'inputs.'\n",
        "\n",
        "        Returns:\n",
        "            dict with (TF placeholder, feed value) as (key, value) pairs.\n",
        "        '''\n",
        "        feed_dict = dict()\n",
        "        feed_dict[self.inputs_bxtxd] = data['inputs']\n",
        "        feed_dict[self.output_bxtxd] = data['output']\n",
        "        return feed_dict\n",
        "\n",
        "    def _get_pred_ops(self):\n",
        "        ''' See docstring in RecurrentWhisperer._get_pred_ops()\n",
        "        '''\n",
        "\n",
        "        return {\n",
        "            'state': self.state_bxtxd,\n",
        "            'output': self.pred_output_bxtxd\n",
        "            }\n",
        "\n",
        "    def _get_batch_size(self, batch_data):\n",
        "        '''See docstring in RecurrentWhisperer.'''\n",
        "        return batch_data['inputs'].shape[0]\n",
        "\n",
        "    def generate_data(self, train_or_valid_str=None):\n",
        "        '''Generates synthetic data (i.e., ground truth trials) for the\n",
        "        FlipFlop task. See comments following FlipFlop class definition for a\n",
        "        description of the input-output relationship in the task.\n",
        "\n",
        "        Args:\n",
        "            None (RecurrentWhisperer option train_or_valid_str is ignored).\n",
        "\n",
        "        Returns:\n",
        "            dict containing 'inputs' and 'outputs'.\n",
        "\n",
        "                'inputs': [n_batch x n_time x n_bits] numpy array containing\n",
        "                input pulses.\n",
        "\n",
        "                'outputs': [n_batch x n_time x n_bits] numpy array specifying\n",
        "                the correct behavior of the FlipFlop memory device.\n",
        "        '''\n",
        "\n",
        "        data_hps = self.hps.data_hps\n",
        "        n_batch = data_hps['n_batch']\n",
        "        n_time = data_hps['n_time']\n",
        "        n_bits = data_hps['n_bits']\n",
        "        p_flip = data_hps['p_flip']\n",
        "\n",
        "        # Randomly generate unsigned input pulses\n",
        "        unsigned_inputs = self.rng.binomial(\n",
        "            1, p_flip, [n_batch, n_time, n_bits])\n",
        "\n",
        "        # Ensure every trial is initialized with a pulse at time 0\n",
        "        unsigned_inputs[:, 0, :] = 1\n",
        "\n",
        "        # Generate random signs {-1, +1}\n",
        "        random_signs = 2*self.rng.binomial(\n",
        "            1, 0.5, [n_batch, n_time, n_bits]) - 1\n",
        "\n",
        "        # Apply random signs to input pulses\n",
        "        inputs = np.multiply(unsigned_inputs, random_signs)\n",
        "\n",
        "        # Allocate output\n",
        "        output = np.zeros([n_batch, n_time, n_bits])\n",
        "\n",
        "        # Update inputs (zero-out random start holds) & compute output\n",
        "        for trial_idx in range(n_batch):\n",
        "            for bit_idx in range(n_bits):\n",
        "                input_ = np.squeeze(inputs[trial_idx, :, bit_idx])\n",
        "                t_flip = np.where(input_ != 0)\n",
        "                for flip_idx in range(np.size(t_flip)):\n",
        "                    # Get the time of the next flip\n",
        "                    t_flip_i = t_flip[0][flip_idx]\n",
        "\n",
        "                    '''Set the output to the sign of the flip for the\n",
        "                    remainder of the trial. Future flips will overwrite future\n",
        "                    output'''\n",
        "                    output[trial_idx, t_flip_i:, bit_idx] = \\\n",
        "                        inputs[trial_idx, t_flip_i, bit_idx]\n",
        "\n",
        "        return {'inputs': inputs, 'output': output}\n",
        "\n",
        "    def _split_data_into_batches(self, data):\n",
        "        '''See docstring in RecurrentWhisperer.'''\n",
        "\n",
        "        # Just use a single batch in this simple example.\n",
        "        return [data], None\n",
        "\n",
        "    def _combine_prediction_batches(self, pred_list, summary_list, idx_list):\n",
        "        '''See docstring in RecurrentWhisperer.'''\n",
        "\n",
        "        # Just use a single batch in this simple example.\n",
        "\n",
        "        assert (len(pred_list)==1),\\\n",
        "            ('FlipFlop only supports single batches, but found %d batches.'\n",
        "             % len(pred_list))\n",
        "\n",
        "        assert (len(summary_list)==1),\\\n",
        "            ('FlipFlop only supports single batches, but found %d batches.'\n",
        "             % len(summary_list))\n",
        "\n",
        "        return pred_list[0], summary_list[0]\n",
        "\n",
        "    def _update_visualizations(self, data, pred,\n",
        "        train_or_valid_str=None,\n",
        "        version=None):\n",
        "        '''See docstring in RecurrentWhisperer.'''\n",
        "\n",
        "        self.plot_trials(data, pred)\n",
        "        self.refresh_figs()\n",
        "\n",
        "    def plot_trials(self, data, pred, start_time=0, stop_time=None):\n",
        "        '''Plots example trials, complete with input pulses, correct outputs,\n",
        "        and RNN-predicted outputs.\n",
        "\n",
        "        Args:\n",
        "            data: dict as returned by generate_flipflop_trials.\n",
        "\n",
        "            start_time (optional): int specifying the first timestep to plot.\n",
        "            Default: 0.\n",
        "\n",
        "            stop_time (optional): int specifying the last timestep to plot.\n",
        "            Default: n_time.\n",
        "\n",
        "        Returns:\n",
        "            None.\n",
        "        '''\n",
        "\n",
        "        FIG_WIDTH = 6 # inches\n",
        "        FIG_HEIGHT = 3 # inches\n",
        "\n",
        "        fig = self._get_fig('example_trials',\n",
        "            width=FIG_WIDTH,\n",
        "            height=FIG_HEIGHT)\n",
        "\n",
        "        hps = self.hps\n",
        "        n_batch = self.hps.data_hps['n_batch']\n",
        "        n_time = self.hps.data_hps['n_time']\n",
        "        n_plot = np.min([hps.n_trials_plot, n_batch])\n",
        "\n",
        "        inputs = data['inputs']\n",
        "        output = data['output']\n",
        "        pred_output = pred['output']\n",
        "\n",
        "        if stop_time is None:\n",
        "            stop_time = n_time\n",
        "\n",
        "        time_idx = range(start_time, stop_time)\n",
        "\n",
        "        for trial_idx in range(n_plot):\n",
        "            ax = plt.subplot(n_plot, 1, trial_idx+1)\n",
        "            if n_plot == 1:\n",
        "                plt.title('Example trial', fontweight='bold')\n",
        "            else:\n",
        "                plt.title('Example trial %d' % (trial_idx + 1),\n",
        "                          fontweight='bold')\n",
        "\n",
        "            self._plot_single_trial(\n",
        "                inputs[trial_idx, time_idx, :],\n",
        "                output[trial_idx, time_idx, :],\n",
        "                pred_output[trial_idx, time_idx, :])\n",
        "\n",
        "            # Only plot x-axis ticks and labels on the bottom subplot\n",
        "            if trial_idx < (n_plot-1):\n",
        "                plt.xticks([])\n",
        "            else:\n",
        "                plt.xlabel('Timestep', fontweight='bold')\n",
        "\n",
        "    @staticmethod\n",
        "    def _plot_single_trial(input_txd, output_txd, pred_output_txd):\n",
        "\n",
        "        VERTICAL_SPACING = 2.5\n",
        "        [n_time, n_bits] = input_txd.shape\n",
        "        tt = range(n_time)\n",
        "\n",
        "        y_ticks = [VERTICAL_SPACING*bit_idx for bit_idx in range(n_bits)]\n",
        "        y_tick_labels = \\\n",
        "            ['Bit %d' % (n_bits-bit_idx) for bit_idx in range(n_bits)]\n",
        "\n",
        "        plt.yticks(y_ticks, y_tick_labels, fontweight='bold')\n",
        "        for bit_idx in range(n_bits):\n",
        "\n",
        "            vertical_offset = VERTICAL_SPACING*bit_idx\n",
        "\n",
        "            # Input pulses\n",
        "            plt.fill_between(\n",
        "                tt,\n",
        "                vertical_offset + input_txd[:, bit_idx],\n",
        "                vertical_offset,\n",
        "                step='mid',\n",
        "                color='gray')\n",
        "\n",
        "            # Correct outputs\n",
        "            plt.step(\n",
        "                tt,\n",
        "                vertical_offset + output_txd[:, bit_idx],\n",
        "                where='mid',\n",
        "                linewidth=2,\n",
        "                color='cyan')\n",
        "\n",
        "            # RNN outputs\n",
        "            plt.step(\n",
        "                tt,\n",
        "                vertical_offset + pred_output_txd[:, bit_idx],\n",
        "                where='mid',\n",
        "                color='purple',\n",
        "                linewidth=1.5,\n",
        "                linestyle='--')\n",
        "\n",
        "        plt.xlim(-1, n_time)"
      ],
      "metadata": {
        "id": "TwjsQOhdbn8t"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_FlipFlop(train_mode):\n",
        "    ''' Train an RNN to solve the N-bit memory task.\n",
        "\n",
        "        Args:\n",
        "            train_mode: 1, 2, or 3.\n",
        "\n",
        "                1.  Generate on-the-fly training data (new data for each\n",
        "                    gradient step)\n",
        "                2.  Provide a single, fixed set of training data.\n",
        "                3.  Provide, single, fixed set of training data (as in 2) and\n",
        "                    a single, fixed set of validation data.\n",
        "\n",
        "                (see docstring to RecurrentWhisperer.train() for more detail)\n",
        "\n",
        "        Returns:\n",
        "            model: FlipFlop object.\n",
        "\n",
        "                The trained RNN model.\n",
        "\n",
        "            valid_predictions: dict.\n",
        "\n",
        "                The model's predictions on a set of held-out validation trials.\n",
        "    '''\n",
        "\n",
        "    assert train_mode in [1, 2, 3], \\\n",
        "        ('train_mode must be 1, 2, or 3, but was %s' % str(train_mode))\n",
        "\n",
        "    # Hyperparameters for FlipFlop\n",
        "    # See FlipFlop.py for detailed descriptions.\n",
        "    hps = {\n",
        "            'rnn_type': 'lstm',\n",
        "            'n_hidden': 16,\n",
        "            'min_loss': 1e-4,\n",
        "            'log_dir': './logs/',\n",
        "            'do_generate_pretraining_visualizations': True,\n",
        "\n",
        "            'data_hps': {\n",
        "                'n_batch': 512,\n",
        "                'n_time': 64,\n",
        "                'n_bits': 3,\n",
        "                'p_flip': 0.5\n",
        "                },\n",
        "\n",
        "            # Hyperparameters for AdaptiveLearningRate\n",
        "            'alr_hps': {\n",
        "                'initial_rate': 1.0,\n",
        "                'min_rate': 1e-5\n",
        "                }\n",
        "            }\n",
        "\n",
        "    model = FlipFlop(**hps)\n",
        "    train_data = model.generate_data()\n",
        "    valid_data = model.generate_data()\n",
        "\n",
        "    if train_mode == 1:\n",
        "        model.train()\n",
        "    elif train_mode == 2:\n",
        "        # This runs much faster at the expense of overfitting potential\n",
        "        model.train(train_data)\n",
        "    elif train_mode == 3:\n",
        "        # This requires some changes to hps to fully leverage validation)\n",
        "        model.train(train_data, valid_data)\n",
        "\n",
        "    # Get example state trajectories from the network\n",
        "    # Visualize inputs, outputs, and RNN predictions from example trials\n",
        "    valid_predictions, valid_summary = model.predict(valid_data)\n",
        "    model.plot_trials(valid_data, valid_predictions)\n",
        "\n",
        "    return model, valid_predictions\n",
        "\n",
        "def find_fixed_points(model, valid_predictions):\n",
        "    ''' Find, analyze, and visualize the fixed points of the trained RNN.\n",
        "\n",
        "    Args:\n",
        "        model: FlipFlop object.\n",
        "\n",
        "            Trained RNN model, as returned by train_FlipFlop().\n",
        "\n",
        "        valid_predictions: dict.\n",
        "\n",
        "            Model predictions on validation trials, as returned by\n",
        "            train_FlipFlop().\n",
        "\n",
        "    Returns:\n",
        "        None.\n",
        "    '''\n",
        "\n",
        "    '''Initial states are sampled from states observed during realistic\n",
        "    behavior of the network. Because a well-trained network transitions\n",
        "    instantaneously from one stable state to another, observed networks states\n",
        "    spend little if any time near the unstable fixed points. In order to\n",
        "    identify ALL fixed points, noise must be added to the initial states\n",
        "    before handing them to the fixed point finder. In this example, the noise\n",
        "    needed is rather large, which can lead to identifying fixed points well\n",
        "    outside of the domain of states observed in realistic behavior of the\n",
        "    network--such fixed points can be safely ignored when interpreting the\n",
        "    dynamical landscape (but can throw visualizations).'''\n",
        "\n",
        "    NOISE_SCALE = 0.5 # Standard deviation of noise added to initial states\n",
        "    N_INITS = 1024 # The number of initial states to provide\n",
        "\n",
        "    n_bits = model.hps.data_hps['n_bits']\n",
        "    is_lstm = model.hps.rnn_type == 'lstm'\n",
        "\n",
        "    '''Fixed point finder hyperparameters. See FixedPointFinder.py for detailed\n",
        "    descriptions of available hyperparameters.'''\n",
        "    fpf_hps = {}\n",
        "\n",
        "    # Setup the fixed point finder\n",
        "    fpf = FixedPointFinder(model.rnn_cell, model.session, **fpf_hps)\n",
        "\n",
        "    # Study the system in the absence of input pulses (e.g., all inputs are 0)\n",
        "    inputs = np.zeros([1,n_bits])\n",
        "\n",
        "    '''Draw random, noise corrupted samples of those state trajectories\n",
        "    to use as initial states for the fixed point optimizations.'''\n",
        "    initial_states = fpf.sample_states(valid_predictions['state'],\n",
        "        n_inits=N_INITS,\n",
        "        noise_scale=NOISE_SCALE)\n",
        "\n",
        "    # Run the fixed point finder\n",
        "    unique_fps, all_fps = fpf.find_fixed_points(initial_states, inputs)\n",
        "\n",
        "    # Visualize identified fixed points with overlaid RNN state trajectories\n",
        "    # All visualized in the 3D PCA space fit the the example RNN states.\n",
        "    plot_fps(unique_fps, valid_predictions['state'],\n",
        "        plot_batch_idx=range(30),\n",
        "        plot_start_time=10)\n",
        "\n",
        "    print('Entering debug mode to allow interaction with objects and figures.')\n",
        "    print('You should see a figure with:')\n",
        "    print('\\tMany blue lines approximately outlining a cube')\n",
        "    print('\\tStable fixed points (black dots) at corners of the cube')\n",
        "    print('\\tUnstable fixed points (red lines or crosses) '\n",
        "        'on edges, surfaces and center of the cube')\n",
        "    print('Enter q to quit.\\n')\n",
        "    pdb.set_trace()\n",
        "\n",
        "def main():\n",
        "\n",
        "    parser = argparse.ArgumentParser(\n",
        "        description='FixedPointFinder: Flip Flop example')\n",
        "    parser.add_argument('--train_mode', default=1, type=int)\n",
        "    args = vars(parser.parse_args())\n",
        "    train_mode = args['train_mode']\n",
        "\n",
        "    # Step 1: Train an RNN to solve the N-bit memory task\n",
        "    model, valid_predictions = train_FlipFlop(train_mode)\n",
        "\n",
        "    # STEP 2: Find, analyze, and visualize the fixed points of the trained RNN\n",
        "    find_fixed_points(model, valid_predictions)\n",
        "\n",
        "# if __name__ == '__main__':\n",
        "#     main()\n"
      ],
      "metadata": {
        "id": "i7l2JGeJcAUh"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  # parser = argparse.ArgumentParser(\n",
        "  #     description='FixedPointFinder: Flip Flop example')\n",
        "  # parser.add_argument('--train_mode', default=1, type=int)\n",
        "  # args = vars(parser.parse_args())\n",
        "  # train_mode = args['train_mode']\n",
        "\n",
        "  # Step 1: Train an RNN to solve the N-bit memory task\n",
        "  model, valid_predictions = train_FlipFlop(2)\n",
        "\n",
        "  # STEP 2: Find, analyze, and visualize the fixed points of the trained RNN\n",
        "  find_fixed_points(model, valid_predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 620
        },
        "id": "Hyh5LChadyAd",
        "outputId": "4b72ca55-475c-4349-c84d-9cb89731ee1c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-89a6ee66214d>:136: UserWarning: `tf.nn.rnn_cell.LSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n",
            "  self.rnn_cell = tf.nn.rnn_cell.LSTMCell(n_hidden)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/keras/layers/rnn/legacy_cells.py:1042: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Run directory found: ./logs/06862fd7da.\n",
            "No checkpoints found.\n",
            "Attempting to build TF model on gpu:0\n",
            "\n",
            "Placing CPU-only ops on cpu:0\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-8486f6f86cf9>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Step 1: Train an RNN to solve the N-bit memory task\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_FlipFlop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# STEP 2: Find, analyze, and visualize the fixed points of the trained RNN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-419f24d5b247>\u001b[0m in \u001b[0;36mtrain_FlipFlop\u001b[0;34m(train_mode)\u001b[0m\n\u001b[1;32m     49\u001b[0m             }\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFlipFlop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mhps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m     \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mvalid_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/recurrent-whisperer/RecurrentWhisperer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data_specs, **kwargs)\u001b[0m\n\u001b[1;32m    440\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_setup_model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 442\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setup_optimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    443\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_setup_optimizer'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/recurrent-whisperer/RecurrentWhisperer.py\u001b[0m in \u001b[0;36m_setup_optimizer\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1164\u001b[0m                 **self.hps.adam_hps)\n\u001b[1;32m   1165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1166\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzipped_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_setup_visualizations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/optimizers/optimizer.py\u001b[0m in \u001b[0;36mapply_gradients\u001b[0;34m(self, grads_and_vars, name, skip_gradients_aggregation, **kwargs)\u001b[0m\n\u001b[1;32m   1172\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mskip_gradients_aggregation\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mexperimental_aggregate_gradients\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1173\u001b[0m             \u001b[0mgrads_and_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maggregate_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1174\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply_weight_decay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/optimizers/optimizer.py\u001b[0m in \u001b[0;36mapply_gradients\u001b[0;34m(self, grads_and_vars, name)\u001b[0m\n\u001b[1;32m    635\u001b[0m                 \u001b[0;31m# Lift variable creation to init scope to avoid environment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m                 \u001b[0;31m# issues.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 637\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    638\u001b[0m         \u001b[0mgrads_and_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m         \u001b[0mgrads_and_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter_empty_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/optimizers/adam.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, var_list)\u001b[0m\n\u001b[1;32m    129\u001b[0m           \u001b[0mvar_list\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mvariables\u001b[0m \u001b[0mto\u001b[0m \u001b[0mbuild\u001b[0m \u001b[0mAdam\u001b[0m \u001b[0mvariables\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \"\"\"\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_built\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_built\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/optimizers/optimizer.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, var_list)\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_built\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_index_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_ema\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_variables_moving_average\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/optimizers/optimizer.py\u001b[0m in \u001b[0;36m_build_index_dict\u001b[0;34m(self, var_list)\u001b[0m\n\u001b[1;32m    439\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_index_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 441\u001b[0;31m             \u001b[0mvar_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_var_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    442\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_index_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvar_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/optimizers/optimizer.py\u001b[0m in \u001b[0;36m_var_key\u001b[0;34m(self, variable)\u001b[0m\n\u001b[1;32m   1123\u001b[0m             \u001b[0;31m# is added to their handle tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1124\u001b[0m             \u001b[0mvariable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_distributed_container\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_var_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0maggregate_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/optimizers/optimizer.py\u001b[0m in \u001b[0;36m_var_key\u001b[0;34m(self, variable)\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;31m# TODO(b/199214315): replace _unique_id with ref() after fixing ref()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0;31m# issues on AggregatingVariable.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mvariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unique_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_deduplicate_sparse_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'RefVariable' object has no attribute '_unique_id'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wFTTPbR_dzd9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
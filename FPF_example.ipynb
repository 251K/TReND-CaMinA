{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/trendinafrica/Comp_Neuro-ML_course/blob/main/FPF_example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0QeXzwvXFQE",
        "outputId": "0eca099b-0176-4a2a-cb90-4569c6e2e835"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'fixed-point-finder'...\n",
            "remote: Enumerating objects: 782, done.\u001b[K\n",
            "remote: Counting objects: 100% (175/175), done.\u001b[K\n",
            "remote: Compressing objects: 100% (30/30), done.\u001b[K\n",
            "remote: Total 782 (delta 156), reused 149 (delta 145), pack-reused 607\u001b[K\n",
            "Receiving objects: 100% (782/782), 501.80 KiB | 2.20 MiB/s, done.\n",
            "Resolving deltas: 100% (474/474), done.\n",
            "Cloning into 'recurrent-whisperer'...\n",
            "remote: Enumerating objects: 974, done.\u001b[K\n",
            "remote: Counting objects: 100% (43/43), done.\u001b[K\n",
            "remote: Compressing objects: 100% (33/33), done.\u001b[K\n",
            "remote: Total 974 (delta 19), reused 29 (delta 10), pack-reused 931\u001b[K\n",
            "Receiving objects: 100% (974/974), 437.36 KiB | 2.06 MiB/s, done.\n",
            "Resolving deltas: 100% (636/636), done.\n"
          ]
        }
      ],
      "source": [
        "! git clone https://github.com/mattgolub/fixed-point-finder.git\n",
        "! git clone https://github.com/mattgolub/recurrent-whisperer.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install numpy==1.24.3 scipy==1.10.1 scikit-learn==1.2.2 matplotlib==3.7.1 PyYAML==6.0 tensorflow==2.8.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "i0auQVI8jXD0",
        "outputId": "b9a6d9bd-4437-4b52-a8b5-f80197928c55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting numpy==1.24.3\n",
            "  Downloading numpy-1.24.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy==1.10.1 in /usr/local/lib/python3.10/dist-packages (1.10.1)\n",
            "Requirement already satisfied: scikit-learn==1.2.2 in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: matplotlib==3.7.1 in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: PyYAML==6.0 in /usr/local/lib/python3.10/dist-packages (6.0)\n",
            "Collecting tensorflow==2.8.0\n",
            "  Downloading tensorflow-2.8.0-cp310-cp310-manylinux2010_x86_64.whl (497.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m497.6/497.6 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.2.2) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.2.2) (3.1.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.1) (1.0.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.1) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.1) (4.39.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.1) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.1) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.1) (8.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.1) (2.8.2)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (23.3.3)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (3.8.0)\n",
            "Collecting keras-preprocessing>=1.1.1 (from tensorflow==2.8.0)\n",
            "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (16.0.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (3.3.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (4.5.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (1.14.1)\n",
            "Collecting tensorboard<2.9,>=2.8 (from tensorflow==2.8.0)\n",
            "  Downloading tensorboard-2.8.0-py3-none-any.whl (5.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m65.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tf-estimator-nightly==2.8.0.dev2021122109 (from tensorflow==2.8.0)\n",
            "  Downloading tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m462.5/462.5 kB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting keras<2.9,>=2.8.0rc0 (from tensorflow==2.8.0)\n",
            "  Downloading keras-2.8.0-py2.py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m79.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (0.32.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (1.54.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.8.0) (0.40.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (2.17.3)\n",
            "Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.9,>=2.8->tensorflow==2.8.0)\n",
            "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (3.4.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (2.27.1)\n",
            "Collecting tensorboard-data-server<0.7.0,>=0.6.0 (from tensorboard<2.9,>=2.8->tensorflow==2.8.0)\n",
            "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m68.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (2.3.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (1.3.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=0.11.15->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (2.1.2)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (3.2.2)\n",
            "Installing collected packages: tf-estimator-nightly, keras, tensorboard-data-server, numpy, keras-preprocessing, google-auth-oauthlib, tensorboard, tensorflow\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.12.0\n",
            "    Uninstalling keras-2.12.0:\n",
            "      Successfully uninstalled keras-2.12.0\n",
            "  Attempting uninstall: tensorboard-data-server\n",
            "    Found existing installation: tensorboard-data-server 0.7.0\n",
            "    Uninstalling tensorboard-data-server-0.7.0:\n",
            "      Successfully uninstalled tensorboard-data-server-0.7.0\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.22.4\n",
            "    Uninstalling numpy-1.22.4:\n",
            "      Successfully uninstalled numpy-1.22.4\n",
            "  Attempting uninstall: google-auth-oauthlib\n",
            "    Found existing installation: google-auth-oauthlib 1.0.0\n",
            "    Uninstalling google-auth-oauthlib-1.0.0:\n",
            "      Successfully uninstalled google-auth-oauthlib-1.0.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.12.2\n",
            "    Uninstalling tensorboard-2.12.2:\n",
            "      Successfully uninstalled tensorboard-2.12.2\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.12.0\n",
            "    Uninstalling tensorflow-2.12.0:\n",
            "      Successfully uninstalled tensorflow-2.12.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "numba 0.56.4 requires numpy<1.24,>=1.18, but you have numpy 1.24.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed google-auth-oauthlib-0.4.6 keras-2.8.0 keras-preprocessing-1.1.2 numpy-1.24.3 tensorboard-2.8.0 tensorboard-data-server-0.6.1 tensorflow-2.8.0 tf-estimator-nightly-2.8.0.dev2021122109\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "run_flipflop.py\n",
        "Written for Python 3.6.9 and TensorFlow 2.8.0\n",
        "@ Matt Golub, October 2018\n",
        "Please direct correspondence to mgolub@cs.washington.edu\n",
        "'''\n",
        "\n",
        "import sys, os\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "jaypvnCK364y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "addpath = lambda dir: sys.path.insert(0, os.path.join('/content/', dir))\n",
        "addpath('recurrent-whisperer')\n",
        "addpath('fixed-point-finder')\n",
        "addpath('fixed-point-finder/example')"
      ],
      "metadata": {
        "id": "D3ofOFHC3mBD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from FlipFlop import FlipFlop\n",
        "from FixedPointFinder import FixedPointFinder\n",
        "from FixedPoints import FixedPoints\n",
        "from plot_utils import plot_fps"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "peGYaKaf5qGv",
        "outputId": "2d30edb6-a06d-417c-9920-157286493008"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No display found. Using non-interactive Agg backend.\n",
            "No display found. Using non-interactive Agg backend.\n",
            "No display found. Using non-interactive Agg backend.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_FlipFlop(train_mode):\n",
        "    ''' Train an RNN to solve the N-bit memory task.\n",
        "\n",
        "        Args:\n",
        "            train_mode: 1, 2, or 3.\n",
        "\n",
        "                1.  Generate on-the-fly training data (new data for each\n",
        "                    gradient step)\n",
        "                2.  Provide a single, fixed set of training data.\n",
        "                3.  Provide, single, fixed set of training data (as in 2) and\n",
        "                    a single, fixed set of validation data.\n",
        "\n",
        "                (see docstring to RecurrentWhisperer.train() for more detail)\n",
        "\n",
        "        Returns:\n",
        "            model: FlipFlop object.\n",
        "\n",
        "                The trained RNN model.\n",
        "\n",
        "            valid_predictions: dict.\n",
        "\n",
        "                The model's predictions on a set of held-out validation trials.\n",
        "    '''\n",
        "\n",
        "    assert train_mode in [1, 2, 3], \\\n",
        "        ('train_mode must be 1, 2, or 3, but was %s' % str(train_mode))\n",
        "\n",
        "    # Hyperparameters for FlipFlop\n",
        "    # See FlipFlop.py for detailed descriptions.\n",
        "    hps = {\n",
        "            'rnn_type': 'lstm',\n",
        "            'n_hidden': 16,\n",
        "            'min_loss': 1e-4,\n",
        "            'log_dir': './logs/',\n",
        "            'do_generate_pretraining_visualizations': True,\n",
        "\n",
        "            'data_hps': {\n",
        "                'n_batch': 512,\n",
        "                'n_time': 64,\n",
        "                'n_bits': 3,\n",
        "                'p_flip': 0.5\n",
        "                },\n",
        "\n",
        "            # Hyperparameters for AdaptiveLearningRate\n",
        "            'alr_hps': {\n",
        "                'initial_rate': 1.0,\n",
        "                'min_rate': 1e-5\n",
        "                }\n",
        "            }\n",
        "\n",
        "    model = FlipFlop(**hps)\n",
        "\n",
        "    train_data = model.generate_data()\n",
        "    valid_data = model.generate_data()\n",
        "\n",
        "    if train_mode == 1:\n",
        "        model.train()\n",
        "    elif train_mode == 2:\n",
        "        # This runs much faster at the expense of overfitting potential\n",
        "        model.train(train_data)\n",
        "    elif train_mode == 3:\n",
        "        # This requires some changes to hps to fully leverage validation\n",
        "        model.train(train_data, valid_data)\n",
        "\n",
        "    # Get example state trajectories from the network\n",
        "    # Visualize inputs, outputs, and RNN predictions from example trials\n",
        "    valid_predictions, valid_summary = model.predict(valid_data)\n",
        "    model.plot_trials(valid_data, valid_predictions)\n",
        "\n",
        "    return model, valid_predictions\n",
        "\n",
        "def find_fixed_points(model, valid_predictions):\n",
        "    ''' Find, analyze, and visualize the fixed points of the trained RNN.\n",
        "\n",
        "    Args:\n",
        "        model: FlipFlop object.\n",
        "\n",
        "            Trained RNN model, as returned by train_FlipFlop().\n",
        "\n",
        "        valid_predictions: dict.\n",
        "\n",
        "            Model predictions on validation trials, as returned by\n",
        "            train_FlipFlop().\n",
        "\n",
        "    Returns:\n",
        "        None.\n",
        "    '''\n",
        "\n",
        "    '''Initial states are sampled from states observed during realistic\n",
        "    behavior of the network. Because a well-trained network transitions\n",
        "    instantaneously from one stable state to another, observed networks states\n",
        "    spend little if any time near the unstable fixed points. In order to\n",
        "    identify ALL fixed points, noise must be added to the initial states\n",
        "    before handing them to the fixed point finder. In this example, the noise\n",
        "    needed is rather large, which can lead to identifying fixed points well\n",
        "    outside of the domain of states observed in realistic behavior of the\n",
        "    network--such fixed points can be safely ignored when interpreting the\n",
        "    dynamical landscape (but can throw visualizations).'''\n",
        "\n",
        "    NOISE_SCALE = 0.5 # Standard deviation of noise added to initial states\n",
        "    N_INITS = 1024 # The number of initial states to provide\n",
        "\n",
        "    n_bits = model.hps.data_hps['n_bits']\n",
        "    is_lstm = model.hps.rnn_type == 'lstm'\n",
        "\n",
        "    '''Fixed point finder hyperparameters. See FixedPointFinder.py for detailed\n",
        "    descriptions of available hyperparameters.'''\n",
        "    fpf_hps = {}\n",
        "\n",
        "    # Setup the fixed point finder\n",
        "    fpf = FixedPointFinder(model.rnn_cell, model.session, **fpf_hps)\n",
        "\n",
        "    # Study the system in the absence of input pulses (e.g., all inputs are 0)\n",
        "    inputs = np.zeros([1,n_bits])\n",
        "\n",
        "    '''Draw random, noise corrupted samples of those state trajectories\n",
        "    to use as initial states for the fixed point optimizations.'''\n",
        "    initial_states = fpf.sample_states(valid_predictions['state'],\n",
        "        n_inits=N_INITS,\n",
        "        noise_scale=NOISE_SCALE)\n",
        "\n",
        "    # Run the fixed point finder\n",
        "    unique_fps, all_fps = fpf.find_fixed_points(initial_states, inputs)\n",
        "\n",
        "    # Visualize identified fixed points with overlaid RNN state trajectories\n",
        "    # All visualized in the 3D PCA space fit the the example RNN states.\n",
        "    fig = plot_fps(unique_fps, valid_predictions['state'],\n",
        "        plot_batch_idx=list(range(30)),\n",
        "        plot_start_time=10)"
      ],
      "metadata": {
        "id": "Yd28eHP_XF7m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_mode = 1\n",
        "\n",
        "# Step 1: Train an RNN to solve the N-bit memory task\n",
        "model, valid_predictions = train_FlipFlop(train_mode)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s9Y7xEq_Ypnu",
        "outputId": "5c22b52c-54be-4b17-9d47-61a46dfee341"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/fixed-point-finder/example/FlipFlop.py:157: UserWarning: `tf.nn.rnn_cell.LSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n",
            "  self.rnn_cell = tf1.nn.rnn_cell.LSTMCell(n_hidden)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/layers/legacy_rnn/rnn_cell_impl.py:984: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
            "  self._kernel = self.add_variable(\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/keras/layers/legacy_rnn/rnn_cell_impl.py:992: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Creating run directory: ./logs/06862fd7da.\n",
            "Attempting to build TF model on gpu:0\n",
            "\n",
            "Placing CPU-only ops on cpu:0\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/layers/legacy_rnn/rnn_cell_impl.py:993: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
            "  self._bias = self.add_variable(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Initializing new run (06862fd7da).\n",
            "\n",
            "Trainable variables:\n",
            "\tRecurrentWhisperer/lstm_cell/kernel:0: (19, 64)\n",
            "\tRecurrentWhisperer/lstm_cell/bias:0: (64,)\n",
            "\tRecurrentWhisperer/W_out:0: (16, 3)\n",
            "\tRecurrentWhisperer/b_out:0: (3,)\n",
            "\n",
            "\n",
            "Total run time time: 3.46s. \n",
            "\t0.0% (986us): setup_hps\n",
            "\t0.1% (4.21ms): _setup_run_dir\n",
            "\t0.0% (1.15ms): set_random_seed\n",
            "\t0.0% (172us): init AdaptiveLearningRate\n",
            "\t0.0% (15.5us): init AdaptiveGradNormClip\n",
            "\t1.8% (62.8ms): _setup_records\n",
            "\t33.2% (1.15s): _setup_model\n",
            "\t49.9% (1.73s): _setup_optimizer\n",
            "\t0.0% (77.0us): _setup_visualizations\n",
            "\t5.1% (177ms): _setup_tensorboard\n",
            "\t2.4% (81.5ms): _setup_savers\n",
            "\t0.0% (1.51ms): _setup_session\n",
            "\t7.3% (254ms): initialize_or_restore\n",
            "\n",
            "\tUpdating Tensorboard images.\n",
            "Entering training loop.\n",
            "Epoch 1 (step 1):\n",
            "\tLearning rate: 1.00e+00\n",
            "\tTraining loss: 1.03e+00\n",
            "\tImprovement: nan\n",
            "\tLogging to: ./logs/06862fd7da\n",
            "\tEpoch time: 3.78s. [ prep data: 4.1% (155ms); batching: 0.0% (16.2us); train: 94.8% (3.59s); ltl: 1.1% (39.9ms); lvl: 0.0% (12.6us); visualize: 0.0% (365us); terminate: 0.0% (102us); ]\n",
            "\n",
            "Epoch 2 (step 2):\n",
            "\tLearning rate: 1.00e+00\n",
            "\tTraining loss: 8.43e-01\n",
            "\tImprovement: 1.83e-01\n",
            "\tLogging to: ./logs/06862fd7da\n",
            "\tEpoch time: 121ms. [ prep data: 62.4% (75.3ms); batching: 0.0% (12.9us); train: 35.7% (43.1ms); ltl: 0.7% (823us); lvl: 0.0% (9.78us); visualize: 0.2% (194us); terminate: 0.1% (88.5us); ]\n",
            "\n",
            "Epoch 3 (step 3):\n",
            "\tLearning rate: 1.00e+00\n",
            "\tTraining loss: 7.44e-01\n",
            "\tImprovement: 9.88e-02\n",
            "\tLogging to: ./logs/06862fd7da\n",
            "\tEpoch time: 126ms. [ prep data: 64.5% (81.1ms); batching: 0.0% (12.4us); train: 34.4% (43.3ms); ltl: 0.7% (854us); lvl: 0.0% (8.34us); visualize: 0.2% (196us); terminate: 0.1% (85.6us); ]\n",
            "\n",
            "Epoch 4 (step 4):\n",
            "\tLearning rate: 1.00e+00\n",
            "\tTraining loss: 6.09e-01\n",
            "\tImprovement: 1.36e-01\n",
            "\tLogging to: ./logs/06862fd7da\n",
            "\tEpoch time: 135ms. [ prep data: 66.6% (89.7ms); batching: 0.0% (12.4us); train: 31.8% (42.9ms); ltl: 0.5% (705us); lvl: 0.0% (6.68us); visualize: 0.1% (141us); terminate: 0.1% (83.2us); ]\n",
            "\n",
            "Epoch 5 (step 5):\n",
            "\tLearning rate: 1.00e+00\n",
            "\tTraining loss: 5.07e-01\n",
            "\tImprovement: 1.02e-01\n",
            "\tLogging to: ./logs/06862fd7da\n",
            "\tEpoch time: 157ms. [ prep data: 67.2% (106ms); batching: 0.0% (12.4us); train: 31.2% (49.0ms); ltl: 1.0% (1.57ms); lvl: 0.0% (10.0us); visualize: 0.2% (258us); terminate: 0.1% (97.8us); ]\n",
            "\n",
            "Epoch 6 (step 6):\n",
            "\tLearning rate: 1.00e+00\n",
            "\tTraining loss: 4.36e-01\n",
            "\tImprovement: 7.11e-02\n",
            "\tLogging to: ./logs/06862fd7da\n",
            "\tEpoch time: 175ms. [ prep data: 75.4% (132ms); batching: 0.0% (9.78us); train: 23.4% (40.9ms); ltl: 0.4% (786us); lvl: 0.0% (9.78us); visualize: 0.1% (184us); terminate: 0.0% (81.5us); ]\n",
            "\n",
            "Epoch 7 (step 7):\n",
            "\tLearning rate: 1.05e+00\n",
            "\tTraining loss: 3.89e-01\n",
            "\tImprovement: 4.61e-02\n",
            "\tLogging to: ./logs/06862fd7da\n",
            "\tEpoch time: 170ms. [ prep data: 71.0% (120ms); batching: 0.0% (11.7us); train: 27.4% (46.4ms); ltl: 0.7% (1.16ms); lvl: 0.0% (9.30us); visualize: 0.1% (217us); terminate: 0.1% (91.6us); ]\n",
            "\n",
            "Epoch 8 (step 8):\n",
            "\tLearning rate: 1.05e+00\n",
            "\tTraining loss: 3.82e-01\n",
            "\tImprovement: 7.22e-03\n",
            "\tLogging to: ./logs/06862fd7da\n",
            "\tEpoch time: 135ms. [ prep data: 67.4% (90.8ms); batching: 0.0% (11.9us); train: 31.1% (41.9ms); ltl: 0.5% (621us); lvl: 0.0% (6.91us); visualize: 0.1% (134us); terminate: 0.1% (116us); ]\n",
            "\n",
            "Epoch 9 (step 9):\n",
            "\tLearning rate: 1.05e+00\n",
            "\tTraining loss: 3.77e-01\n",
            "\tImprovement: 5.42e-03\n",
            "\tLogging to: ./logs/06862fd7da\n",
            "\tEpoch time: 142ms. [ prep data: 69.2% (98.2ms); batching: 0.0% (10.0us); train: 29.1% (41.3ms); ltl: 0.7% (1.05ms); lvl: 0.0% (9.78us); visualize: 0.2% (256us); terminate: 0.1% (95.6us); ]\n",
            "\n",
            "Epoch 10 (step 10):\n",
            "\tLearning rate: 1.05e+00\n",
            "\tTraining loss: 4.01e-01\n",
            "\tImprovement: -2.41e-02\n",
            "\tLogging to: ./logs/06862fd7da\n",
            "\tEpoch time: 142ms. [ prep data: 68.6% (97.1ms); batching: 0.0% (11.2us); train: 29.9% (42.3ms); ltl: 0.7% (990us); lvl: 0.0% (10.0us); visualize: 0.1% (210us); terminate: 0.1% (107us); ]\n",
            "\n",
            "Epoch 11 (step 11):\n",
            "\tLearning rate: 1.05e+00\n",
            "\tTraining loss: 3.78e-01\n",
            "\tImprovement: 2.28e-02\n",
            "\tLogging to: ./logs/06862fd7da\n",
            "\tEpoch time: 185ms. [ prep data: 71.1% (132ms); batching: 0.0% (13.1us); train: 25.8% (47.7ms); ltl: 1.7% (3.17ms); lvl: 0.0% (11.9us); visualize: 0.2% (422us); terminate: 0.1% (113us); ]\n",
            "\n",
            "Epoch 12 (step 12):\n",
            "\tLearning rate: 1.05e+00\n",
            "\tTraining loss: 3.72e-01\n",
            "\tImprovement: 6.62e-03\n",
            "\tLogging to: ./logs/06862fd7da\n",
            "\tEpoch time: 155ms. [ prep data: 66.8% (103ms); batching: 0.0% (11.7us); train: 31.0% (48.0ms); ltl: 1.1% (1.64ms); lvl: 0.0% (8.82us); visualize: 0.2% (305us); terminate: 0.1% (111us); ]\n",
            "\n",
            "Epoch 13 (step 13):\n",
            "\tLearning rate: 1.05e+00\n",
            "\tTraining loss: 3.60e-01\n",
            "\tImprovement: 1.14e-02\n",
            "\tLogging to: ./logs/06862fd7da\n",
            "\tEpoch time: 157ms. [ prep data: 73.7% (116ms); batching: 0.0% (10.5us); train: 24.8% (39.0ms); ltl: 0.6% (1.00ms); lvl: 0.0% (8.11us); visualize: 0.2% (243us); terminate: 0.1% (78.9us); ]\n",
            "\n",
            "Epoch 14 (step 14):\n",
            "\tLearning rate: 1.05e+00\n",
            "\tTraining loss: 3.44e-01\n",
            "\tImprovement: 1.65e-02\n",
            "\tLogging to: ./logs/06862fd7da\n",
            "\tEpoch time: 176ms. [ prep data: 74.2% (131ms); batching: 0.0% (11.0us); train: 24.5% (43.2ms); ltl: 1.0% (1.82ms); lvl: 0.0% (9.06us); visualize: 0.2% (280us); terminate: 0.0% (79.2us); ]\n",
            "\n",
            "Epoch 15 (step 15):\n",
            "\tLearning rate: 1.05e+00\n",
            "\tTraining loss: 3.41e-01\n",
            "\tImprovement: 2.55e-03\n",
            "\tLogging to: ./logs/06862fd7da\n",
            "\tEpoch time: 208ms. [ prep data: 68.4% (142ms); batching: 0.0% (16.0us); train: 30.5% (63.5ms); ltl: 0.5% (1.08ms); lvl: 0.0% (9.30us); visualize: 0.1% (248us); terminate: 0.0% (91.8us); ]\n",
            "\n",
            "Epoch 16 (step 16):\n",
            "\tLearning rate: 1.11e+00\n",
            "\tTraining loss: 3.26e-01\n",
            "\tImprovement: 1.47e-02\n",
            "\tLogging to: ./logs/06862fd7da\n",
            "\tEpoch time: 238ms. [ prep data: 69.1% (164ms); batching: 0.0% (16.0us); train: 29.3% (69.7ms); ltl: 0.5% (1.21ms); lvl: 0.0% (9.54us); visualize: 0.1% (297us); terminate: 0.0% (107us); ]\n",
            "\n",
            "Epoch 17 (step 17):\n",
            "\tLearning rate: 1.11e+00\n",
            "\tTraining loss: 2.96e-01\n",
            "\tImprovement: 3.05e-02\n",
            "\tLogging to: ./logs/06862fd7da\n",
            "\tEpoch time: 273ms. [ prep data: 71.6% (196ms); batching: 0.0% (15.0us); train: 27.5% (75.0ms); ltl: 0.4% (1.18ms); lvl: 0.0% (9.30us); visualize: 0.1% (267us); terminate: 0.0% (102us); ]\n",
            "\n",
            "Epoch 18 (step 18):\n",
            "\tLearning rate: 1.11e+00\n",
            "\tTraining loss: 2.80e-01\n",
            "\tImprovement: 1.57e-02\n",
            "\tLogging to: ./logs/06862fd7da\n",
            "\tEpoch time: 268ms. [ prep data: 69.4% (186ms); batching: 0.0% (14.3us); train: 29.2% (78.4ms); ltl: 0.5% (1.24ms); lvl: 0.0% (9.30us); visualize: 0.1% (313us); terminate: 0.0% (88.7us); ]\n",
            "\n",
            "Epoch 19 (step 19):\n",
            "\tLearning rate: 1.11e+00\n",
            "\tTraining loss: 2.55e-01\n",
            "\tImprovement: 2.53e-02\n",
            "\tLogging to: ./logs/06862fd7da\n",
            "\tEpoch time: 228ms. [ prep data: 64.9% (148ms); batching: 0.0% (14.8us); train: 33.7% (76.7ms); ltl: 0.5% (1.06ms); lvl: 0.0% (8.34us); visualize: 0.1% (269us); terminate: 0.0% (89.9us); ]\n",
            "\n",
            "Epoch 20 (step 20):\n",
            "\tLearning rate: 1.11e+00\n",
            "\tTraining loss: 2.32e-01\n",
            "\tImprovement: 2.32e-02\n",
            "\tLogging to: ./logs/06862fd7da\n",
            "\tEpoch time: 225ms. [ prep data: 65.0% (147ms); batching: 0.0% (14.1us); train: 33.3% (75.1ms); ltl: 0.5% (1.22ms); lvl: 0.0% (10.5us); visualize: 0.1% (265us); terminate: 0.0% (107us); ]\n",
            "\n",
            "Epoch 21 (step 21):\n",
            "\tLearning rate: 1.17e+00\n",
            "\tTraining loss: 2.19e-01\n",
            "\tImprovement: 1.25e-02\n",
            "\tLogging to: ./logs/06862fd7da\n",
            "\tEpoch time: 267ms. [ prep data: 65.9% (176ms); batching: 0.0% (14.8us); train: 31.4% (83.8ms); ltl: 0.5% (1.20ms); lvl: 0.0% (78.7us); visualize: 0.1% (310us); terminate: 0.0% (120us); ]\n",
            "\n",
            "Epoch 22 (step 22):\n",
            "\tLearning rate: 1.17e+00\n",
            "\tTraining loss: 1.91e-01\n",
            "\tImprovement: 2.79e-02\n",
            "\tLogging to: ./logs/06862fd7da\n",
            "\tEpoch time: 254ms. [ prep data: 65.8% (167ms); batching: 0.0% (16.0us); train: 32.6% (83.0ms); ltl: 0.6% (1.62ms); lvl: 0.0% (9.78us); visualize: 0.1% (266us); terminate: 0.0% (94.7us); ]\n",
            "\n",
            "Epoch 23 (step 23):\n",
            "\tLearning rate: 1.17e+00\n",
            "\tTraining loss: 1.73e-01\n",
            "\tImprovement: 1.82e-02\n",
            "\tLogging to: ./logs/06862fd7da\n",
            "\tEpoch time: 222ms. [ prep data: 63.1% (140ms); batching: 0.0% (15.0us); train: 35.4% (78.6ms); ltl: 0.5% (1.08ms); lvl: 0.0% (9.06us); visualize: 0.1% (240us); terminate: 0.0% (91.1us); ]\n",
            "\n",
            "Epoch 24 (step 24):\n",
            "\tLearning rate: 1.17e+00\n",
            "\tTraining loss: 1.47e-01\n",
            "\tImprovement: 2.61e-02\n",
            "\tLogging to: ./logs/06862fd7da\n",
            "\tEpoch time: 226ms. [ prep data: 64.1% (145ms); batching: 0.0% (15.7us); train: 34.6% (78.1ms); ltl: 0.4% (938us); lvl: 0.0% (10.3us); visualize: 0.1% (194us); terminate: 0.0% (88.9us); ]\n",
            "\n",
            "Epoch 25 (step 25):\n",
            "\tLearning rate: 1.17e+00\n",
            "\tTraining loss: 1.28e-01\n",
            "\tImprovement: 1.91e-02\n",
            "\tLogging to: ./logs/06862fd7da\n",
            "\tEpoch time: 270ms. [ prep data: 68.8% (186ms); batching: 0.0% (14.5us); train: 29.3% (79.2ms); ltl: 0.5% (1.46ms); lvl: 0.0% (9.54us); visualize: 0.1% (268us); terminate: 0.0% (94.4us); ]\n",
            "\n",
            "Epoch 26 (step 26):\n",
            "\tLearning rate: 1.23e+00\n",
            "\tTraining loss: 1.17e-01\n",
            "\tImprovement: 1.10e-02\n",
            "\tLogging to: ./logs/06862fd7da\n",
            "\tEpoch time: 235ms. [ prep data: 62.8% (148ms); batching: 0.0% (15.5us); train: 35.3% (83.0ms); ltl: 0.9% (2.08ms); lvl: 0.0% (9.30us); visualize: 0.1% (307us); terminate: 0.0% (96.1us); ]\n",
            "\n",
            "Epoch 27 (step 27):\n",
            "\tLearning rate: 1.23e+00\n",
            "\tTraining loss: 1.01e-01\n",
            "\tImprovement: 1.56e-02\n",
            "\tLogging to: ./logs/06862fd7da\n",
            "\tEpoch time: 224ms. [ prep data: 60.1% (135ms); batching: 0.0% (14.8us); train: 38.0% (85.2ms); ltl: 0.8% (1.71ms); lvl: 0.0% (10.0us); visualize: 0.1% (325us); terminate: 0.1% (113us); ]\n",
            "\n",
            "Epoch 28 (step 28):\n",
            "\tLearning rate: 1.23e+00\n",
            "\tTraining loss: 8.82e-02\n",
            "\tImprovement: 1.30e-02\n",
            "\tLogging to: ./logs/06862fd7da\n",
            "\tEpoch time: 217ms. [ prep data: 61.0% (133ms); batching: 0.0% (15.3us); train: 37.3% (81.0ms); ltl: 0.6% (1.32ms); lvl: 0.0% (10.0us); visualize: 0.1% (323us); terminate: 0.0% (95.6us); ]\n",
            "\n",
            "Epoch 29 (step 29):\n",
            "\tLearning rate: 1.23e+00\n",
            "\tTraining loss: 7.56e-02\n",
            "\tImprovement: 1.26e-02\n",
            "\tLogging to: ./logs/06862fd7da\n",
            "\tEpoch time: 204ms. [ prep data: 60.2% (123ms); batching: 0.0% (14.1us); train: 38.2% (77.9ms); ltl: 0.5% (1.02ms); lvl: 0.0% (33.4us); visualize: 0.1% (244us); terminate: 0.0% (92.5us); ]\n",
            "\n",
            "Epoch 30 (step 30):\n",
            "\tLearning rate: 1.23e+00\n",
            "\tTraining loss: 6.49e-02\n",
            "\tImprovement: 1.07e-02\n",
            "\tLogging to: ./logs/06862fd7da\n",
            "\tEpoch time: 213ms. [ prep data: 59.9% (127ms); batching: 0.0% (14.3us); train: 38.3% (81.5ms); ltl: 0.6% (1.30ms); lvl: 0.0% (10.3us); visualize: 0.2% (326us); terminate: 0.0% (92.3us); ]\n",
            "\n",
            "Epoch 31 (step 31):\n",
            "\tLearning rate: 1.29e+00\n",
            "\tTraining loss: 5.64e-02\n",
            "\tImprovement: 8.45e-03\n",
            "\tLogging to: ./logs/06862fd7da\n",
            "\tEpoch time: 227ms. [ prep data: 65.7% (149ms); batching: 0.0% (14.1us); train: 32.3% (73.2ms); ltl: 0.9% (1.98ms); lvl: 0.0% (10.7us); visualize: 0.1% (302us); terminate: 0.0% (92.0us); ]\n",
            "\n",
            "Epoch 32 (step 32):\n",
            "\tLearning rate: 1.29e+00\n",
            "\tTraining loss: 4.59e-02\n",
            "\tImprovement: 1.05e-02\n",
            "\tLogging to: ./logs/06862fd7da\n",
            "\tEpoch time: 206ms. [ prep data: 60.1% (124ms); batching: 0.0% (15.0us); train: 37.9% (78.1ms); ltl: 0.7% (1.49ms); lvl: 0.0% (10.0us); visualize: 0.2% (344us); terminate: 0.0% (98.7us); ]\n",
            "\n",
            "Epoch 33 (step 33):\n",
            "\tLearning rate: 1.29e+00\n",
            "\tTraining loss: 3.80e-02\n",
            "\tImprovement: 7.92e-03\n",
            "\tLogging to: ./logs/06862fd7da\n",
            "\tEpoch time: 259ms. [ prep data: 67.3% (175ms); batching: 0.0% (15.7us); train: 31.1% (80.6ms); ltl: 0.5% (1.35ms); lvl: 0.0% (9.78us); visualize: 0.1% (365us); terminate: 0.0% (101us); ]\n",
            "\n",
            "Epoch 34 (step 34):\n",
            "\tLearning rate: 1.29e+00\n",
            "\tTraining loss: 3.10e-02\n",
            "\tImprovement: 7.01e-03\n",
            "\tLogging to: ./logs/06862fd7da\n",
            "\tEpoch time: 260ms. [ prep data: 68.4% (178ms); batching: 0.0% (16.2us); train: 30.1% (78.1ms); ltl: 0.5% (1.19ms); lvl: 0.0% (8.82us); visualize: 0.1% (293us); terminate: 0.0% (98.9us); ]\n",
            "\n",
            "Epoch 35 (step 35):\n",
            "\tLearning rate: 1.29e+00\n",
            "\tTraining loss: 2.31e-02\n",
            "\tImprovement: 7.90e-03\n",
            "\tLogging to: ./logs/06862fd7da\n",
            "\tEpoch time: 225ms. [ prep data: 62.6% (141ms); batching: 0.0% (14.3us); train: 35.6% (80.3ms); ltl: 0.7% (1.51ms); lvl: 0.0% (9.30us); visualize: 0.1% (295us); terminate: 0.0% (93.2us); ]\n",
            "\n",
            "Epoch 36 (step 36):\n",
            "\tLearning rate: 1.36e+00\n",
            "\tTraining loss: 1.86e-02\n",
            "\tImprovement: 4.51e-03\n",
            "\tLogging to: ./logs/06862fd7da\n",
            "\tEpoch time: 205ms. [ prep data: 60.0% (123ms); batching: 0.0% (16.0us); train: 37.6% (77.1ms); ltl: 0.6% (1.29ms); lvl: 0.0% (8.82us); visualize: 0.2% (320us); terminate: 0.1% (112us); ]\n",
            "\n",
            "Epoch 37 (step 37):\n",
            "\tLearning rate: 1.36e+00\n",
            "\tTraining loss: 1.49e-02\n",
            "\tImprovement: 3.68e-03\n",
            "\tLogging to: ./logs/06862fd7da\n",
            "\tEpoch time: 208ms. [ prep data: 65.6% (137ms); batching: 0.0% (40.3us); train: 33.5% (69.8ms); ltl: 0.6% (1.26ms); lvl: 0.0% (11.0us); visualize: 0.2% (345us); terminate: 0.1% (109us); ]\n",
            "\n",
            "Epoch 38 (step 38):\n",
            "\tLearning rate: 1.36e+00\n",
            "\tTraining loss: 1.24e-02\n",
            "\tImprovement: 2.57e-03\n",
            "\tLogging to: ./logs/06862fd7da\n",
            "\tEpoch time: 218ms. [ prep data: 68.3% (149ms); batching: 0.0% (15.5us); train: 29.5% (64.2ms); ltl: 0.5% (1.05ms); lvl: 0.0% (8.82us); visualize: 0.1% (218us); terminate: 0.0% (80.6us); ]\n",
            "\n",
            "Epoch 39 (step 39):\n",
            "\tLearning rate: 1.36e+00\n",
            "\tTraining loss: 1.04e-02\n",
            "\tImprovement: 1.97e-03\n",
            "\tLogging to: ./logs/06862fd7da\n",
            "\tEpoch time: 206ms. [ prep data: 64.5% (133ms); batching: 0.0% (14.5us); train: 34.1% (70.3ms); ltl: 0.7% (1.44ms); lvl: 0.0% (10.3us); visualize: 0.2% (348us); terminate: 0.0% (93.0us); ]\n",
            "\n",
            "Epoch 40 (step 40):\n",
            "\tLearning rate: 1.36e+00\n",
            "\tTraining loss: 8.07e-03\n",
            "\tImprovement: 2.32e-03\n",
            "\tLogging to: ./logs/06862fd7da\n",
            "\tEpoch time: 227ms. [ prep data: 63.2% (143ms); batching: 0.0% (15.5us); train: 34.7% (78.7ms); ltl: 0.7% (1.66ms); lvl: 0.0% (10.7us); visualize: 0.1% (307us); terminate: 0.0% (98.0us); ]\n",
            "\n",
            "Epoch 41 (step 41):\n",
            "\tLearning rate: 1.43e+00\n",
            "\tTraining loss: 7.97e-03\n",
            "\tImprovement: 1.02e-04\n",
            "\tLogging to: ./logs/06862fd7da\n",
            "\tEpoch time: 261ms. [ prep data: 67.9% (177ms); batching: 0.0% (15.3us); train: 30.6% (79.9ms); ltl: 0.5% (1.26ms); lvl: 0.0% (10.0us); visualize: 0.1% (291us); terminate: 0.0% (113us); ]\n",
            "\n",
            "Epoch 42 (step 42):\n",
            "\tLearning rate: 1.43e+00\n",
            "\tTraining loss: 8.27e-03\n",
            "\tImprovement: -3.08e-04\n",
            "\tLogging to: ./logs/06862fd7da\n",
            "\tEpoch time: 154ms. [ prep data: 69.2% (106ms); batching: 0.0% (11.7us); train: 29.3% (45.0ms); ltl: 0.6% (901us); lvl: 0.0% (10.3us); visualize: 0.1% (228us); terminate: 0.1% (83.9us); ]\n",
            "\n",
            "Epoch 43 (step 43):\n",
            "\tLearning rate: 1.43e+00\n",
            "\tTraining loss: 7.11e-03\n",
            "\tImprovement: 1.17e-03\n",
            "\tLogging to: ./logs/06862fd7da\n",
            "\tEpoch time: 189ms. [ prep data: 75.5% (143ms); batching: 0.0% (13.6us); train: 23.0% (43.6ms); ltl: 0.6% (1.05ms); lvl: 0.0% (8.58us); visualize: 0.1% (272us); terminate: 0.0% (79.6us); ]\n",
            "\n",
            "Epoch 44 (step 44):\n",
            "\tLearning rate: 1.43e+00\n",
            "\tTraining loss: 6.38e-03\n",
            "\tImprovement: 7.28e-04\n",
            "\tLogging to: ./logs/06862fd7da\n",
            "\tEpoch time: 146ms. [ prep data: 68.0% (99.1ms); batching: 0.0% (11.2us); train: 30.3% (44.2ms); ltl: 0.6% (936us); lvl: 0.0% (8.34us); visualize: 0.2% (252us); terminate: 0.1% (82.0us); ]\n",
            "\n",
            "Epoch 45 (step 45):\n",
            "\tLearning rate: 1.43e+00\n",
            "\tTraining loss: 5.95e-03\n",
            "\tImprovement: 4.27e-04\n",
            "\tLogging to: ./logs/06862fd7da\n",
            "\tEpoch time: 134ms. [ prep data: 72.3% (97.1ms); batching: 0.0% (13.8us); train: 26.8% (36.0ms); ltl: 0.5% (705us); lvl: 0.0% (9.06us); visualize: 0.1% (197us); terminate: 0.1% (110us); ]\n",
            "\n",
            "Epoch 46 (step 46):\n",
            "\tLearning rate: 1.43e+00\n",
            "\tTraining loss: 5.67e-03\n",
            "\tImprovement: 2.78e-04\n",
            "\tLogging to: ./logs/06862fd7da\n",
            "\tEpoch time: 134ms. [ prep data: 57.2% (76.9ms); batching: 0.0% (14.8us); train: 41.0% (55.1ms); ltl: 0.8% (1.12ms); lvl: 0.0% (10.0us); visualize: 0.1% (190us); terminate: 0.1% (82.0us); ]\n",
            "\n",
            "Epoch 47 (step 47):\n",
            "\tLearning rate: 1.43e+00\n",
            "\tTraining loss: 5.16e-03\n",
            "\tImprovement: 5.20e-04\n",
            "\tLogging to: ./logs/06862fd7da\n",
            "\tEpoch time: 116ms. [ prep data: 58.8% (68.1ms); batching: 0.0% (12.2us); train: 39.1% (45.3ms); ltl: 0.8% (945us); lvl: 0.0% (6.91us); visualize: 0.2% (235us); terminate: 0.1% (75.8us); ]\n",
            "\n",
            "Epoch 48 (step 48):\n",
            "\tLearning rate: 1.51e+00\n",
            "\tTraining loss: 4.09e-03\n",
            "\tImprovement: 1.06e-03\n",
            "\tLogging to: ./logs/06862fd7da\n",
            "\tEpoch time: 127ms. [ prep data: 66.6% (84.3ms); batching: 0.0% (11.0us); train: 31.6% (40.0ms); ltl: 0.8% (1.02ms); lvl: 0.0% (8.58us); visualize: 0.2% (257us); terminate: 0.1% (77.5us); ]\n",
            "\n",
            "Epoch 49 (step 49):\n",
            "\tLearning rate: 1.51e+00\n",
            "\tTraining loss: 4.09e-03\n",
            "\tImprovement: 7.03e-07\n",
            "\tLogging to: ./logs/06862fd7da\n",
            "\tEpoch time: 169ms. [ prep data: 69.7% (118ms); batching: 0.0% (12.4us); train: 28.6% (48.4ms); ltl: 0.7% (1.19ms); lvl: 0.0% (7.87us); visualize: 0.1% (212us); terminate: 0.1% (113us); ]\n",
            "\n",
            "Epoch 50 (step 50):\n",
            "\tLearning rate: 1.51e+00\n",
            "\tTraining loss: 3.92e-03\n",
            "\tImprovement: 1.73e-04\n",
            "\tLogging to: ./logs/06862fd7da\n",
            "\tEpoch time: 171ms. [ prep data: 76.3% (130ms); batching: 0.0% (11.2us); train: 22.6% (38.6ms); ltl: 0.6% (1.00ms); lvl: 0.0% (9.78us); visualize: 0.1% (221us); terminate: 0.0% (85.1us); ]\n",
            "\n",
            "Epoch 51 (step 51):\n",
            "\tLearning rate: 1.51e+00\n",
            "\tTraining loss: 3.36e-03\n",
            "\tImprovement: 5.56e-04\n",
            "\tLogging to: ./logs/06862fd7da\n",
            "\tEpoch time: 156ms. [ prep data: 71.9% (112ms); batching: 0.0% (13.1us); train: 26.5% (41.3ms); ltl: 0.8% (1.31ms); lvl: 0.0% (26.7us); visualize: 0.2% (246us); terminate: 0.1% (86.3us); ]\n",
            "\n",
            "Epoch 52 (step 52):\n",
            "\tLearning rate: 1.51e+00\n",
            "\tTraining loss: 2.88e-03\n",
            "\tImprovement: 4.79e-04\n",
            "\tLogging to: ./logs/06862fd7da\n",
            "\tEpoch time: 108ms. [ prep data: 61.5% (66.5ms); batching: 0.0% (11.4us); train: 37.7% (40.8ms); ltl: 0.5% (516us); lvl: 0.0% (7.87us); visualize: 0.1% (116us); terminate: 0.1% (102us); ]\n",
            "\n",
            "Epoch 53 (step 53):\n",
            "\tLearning rate: 1.59e+00\n",
            "\tTraining loss: 2.99e-03\n",
            "\tImprovement: -1.05e-04\n",
            "\tLogging to: ./logs/06862fd7da\n",
            "\tEpoch time: 124ms. [ prep data: 54.8% (67.7ms); batching: 0.0% (11.2us); train: 42.1% (52.0ms); ltl: 1.0% (1.20ms); lvl: 0.0% (10.0us); visualize: 0.2% (261us); terminate: 0.1% (143us); ]\n",
            "\n",
            "Epoch 54 (step 54):\n",
            "\tLearning rate: 1.59e+00\n",
            "\tTraining loss: 2.49e-03\n",
            "\tImprovement: 4.93e-04\n",
            "\tLogging to: ./logs/06862fd7da\n",
            "\tEpoch time: 111ms. [ prep data: 63.7% (70.9ms); batching: 0.0% (13.4us); train: 34.3% (38.2ms); ltl: 0.7% (811us); lvl: 0.0% (9.54us); visualize: 0.2% (222us); terminate: 0.1% (85.6us); ]\n",
            "\n",
            "Epoch 55 (step 55):\n",
            "\tLearning rate: 1.59e+00\n",
            "\tTraining loss: 2.17e-03\n",
            "\tImprovement: 3.21e-04\n",
            "\tLogging to: ./logs/06862fd7da\n",
            "\tEpoch time: 130ms. [ prep data: 60.1% (78.0ms); batching: 0.0% (11.0us); train: 38.7% (50.2ms); ltl: 0.7% (903us); lvl: 0.0% (9.06us); visualize: 0.2% (224us); terminate: 0.1% (85.6us); ]\n",
            "\n",
            "Epoch 56 (step 56):\n",
            "\tLearning rate: 1.59e+00\n",
            "\tTraining loss: 1.70e-03\n",
            "\tImprovement: 4.74e-04\n",
            "\tLogging to: ./logs/06862fd7da\n",
            "\tEpoch time: 158ms. [ prep data: 70.2% (111ms); batching: 0.0% (13.1us); train: 28.1% (44.6ms); ltl: 0.6% (921us); lvl: 0.0% (7.87us); visualize: 0.1% (136us); terminate: 0.0% (72.5us); ]\n",
            "\n",
            "Epoch 57 (step 57):\n",
            "\tLearning rate: 1.59e+00\n",
            "\tTraining loss: 1.63e-03\n",
            "\tImprovement: 6.87e-05\n",
            "\tLogging to: ./logs/06862fd7da\n",
            "\tEpoch time: 162ms. [ prep data: 68.8% (111ms); batching: 0.0% (11.7us); train: 29.7% (48.2ms); ltl: 0.6% (958us); lvl: 0.0% (7.63us); visualize: 0.1% (123us); terminate: 0.0% (79.6us); ]\n",
            "\n",
            "Epoch 58 (step 58):\n",
            "\tLearning rate: 1.59e+00\n",
            "\tTraining loss: 1.28e-03\n",
            "\tImprovement: 3.53e-04\n",
            "\tLogging to: ./logs/06862fd7da\n",
            "\tEpoch time: 161ms. [ prep data: 71.3% (115ms); batching: 0.0% (14.8us); train: 27.4% (44.1ms); ltl: 0.4% (685us); lvl: 0.0% (7.63us); visualize: 0.1% (138us); terminate: 0.1% (112us); ]\n",
            "\n",
            "Epoch 59 (step 59):\n",
            "\tLearning rate: 1.67e+00\n",
            "\tTraining loss: 1.12e-03\n",
            "\tImprovement: 1.54e-04\n",
            "\tLogging to: ./logs/06862fd7da\n",
            "\tEpoch time: 165ms. [ prep data: 74.2% (122ms); batching: 0.0% (12.4us); train: 24.1% (39.7ms); ltl: 0.8% (1.29ms); lvl: 0.0% (9.30us); visualize: 0.1% (212us); terminate: 0.1% (87.0us); ]\n",
            "\n",
            "Epoch 60 (step 60):\n",
            "\tLearning rate: 1.67e+00\n",
            "\tTraining loss: 1.20e-03\n",
            "\tImprovement: -7.38e-05\n",
            "\tLogging to: ./logs/06862fd7da\n",
            "\tEpoch time: 120ms. [ prep data: 56.0% (67.0ms); batching: 0.0% (13.4us); train: 41.9% (50.1ms); ltl: 0.9% (1.03ms); lvl: 0.0% (10.5us); visualize: 0.2% (245us); terminate: 0.1% (111us); ]\n",
            "\n",
            "Epoch 61 (step 61):\n",
            "\tLearning rate: 1.67e+00\n",
            "\tTraining loss: 9.90e-04\n",
            "\tImprovement: 2.08e-04\n",
            "\tLogging to: ./logs/06862fd7da\n",
            "\tEpoch time: 159ms. [ prep data: 71.0% (112ms); batching: 0.0% (12.2us); train: 27.5% (43.5ms); ltl: 0.6% (950us); lvl: 0.0% (9.54us); visualize: 0.1% (206us); terminate: 0.1% (84.9us); ]\n",
            "\n",
            "Epoch 62 (step 62):\n",
            "\tLearning rate: 1.67e+00\n",
            "\tTraining loss: 1.01e-03\n",
            "\tImprovement: -1.85e-05\n",
            "\tLogging to: ./logs/06862fd7da\n",
            "\tEpoch time: 121ms. [ prep data: 62.7% (75.7ms); batching: 0.3% (318us); train: 35.5% (42.9ms); ltl: 1.1% (1.31ms); lvl: 0.0% (10.0us); visualize: 0.2% (231us); terminate: 0.1% (97.3us); ]\n",
            "\n",
            "Epoch 63 (step 63):\n",
            "\tLearning rate: 1.67e+00\n",
            "\tTraining loss: 1.00e-03\n",
            "\tImprovement: 6.65e-06\n",
            "\tLogging to: ./logs/06862fd7da\n",
            "\tEpoch time: 144ms. [ prep data: 66.8% (96.2ms); batching: 0.0% (11.2us); train: 30.9% (44.5ms); ltl: 0.9% (1.33ms); lvl: 0.0% (9.78us); visualize: 0.1% (170us); terminate: 0.1% (97.0us); ]\n",
            "\n",
            "Epoch 64 (step 64):\n",
            "\tLearning rate: 1.67e+00\n",
            "\tTraining loss: 9.06e-04\n",
            "\tImprovement: 9.52e-05\n",
            "\tLogging to: ./logs/06862fd7da\n",
            "\tEpoch time: 143ms. [ prep data: 72.0% (103ms); batching: 0.0% (12.2us); train: 26.7% (38.1ms); ltl: 0.7% (1.03ms); lvl: 0.0% (7.87us); visualize: 0.2% (220us); terminate: 0.1% (74.4us); ]\n",
            "\n",
            "Epoch 65 (step 65):\n",
            "\tLearning rate: 1.67e+00\n",
            "\tTraining loss: 7.75e-04\n",
            "\tImprovement: 1.32e-04\n",
            "\tLogging to: ./logs/06862fd7da\n",
            "\tEpoch time: 134ms. [ prep data: 63.9% (85.7ms); batching: 0.0% (11.9us); train: 34.3% (46.0ms); ltl: 0.5% (686us); lvl: 0.0% (6.91us); visualize: 0.1% (166us); terminate: 0.1% (76.8us); ]\n",
            "\n",
            "Epoch 66 (step 66):\n",
            "\tLearning rate: 1.67e+00\n",
            "\tTraining loss: 7.22e-04\n",
            "\tImprovement: 5.28e-05\n",
            "\tLogging to: ./logs/06862fd7da\n",
            "\tEpoch time: 157ms. [ prep data: 74.6% (118ms); batching: 0.0% (11.0us); train: 24.3% (38.2ms); ltl: 0.7% (1.17ms); lvl: 0.0% (9.30us); visualize: 0.2% (249us); terminate: 0.0% (78.4us); ]\n",
            "\n",
            "Epoch 67 (step 67):\n",
            "\tLearning rate: 1.67e+00\n",
            "\tTraining loss: 7.12e-04\n",
            "\tImprovement: 9.78e-06\n",
            "\tLogging to: ./logs/06862fd7da\n",
            "\tEpoch time: 161ms. [ prep data: 66.8% (107ms); batching: 0.0% (16.0us); train: 32.2% (51.8ms); ltl: 0.7% (1.14ms); lvl: 0.0% (7.15us); visualize: 0.1% (215us); terminate: 0.1% (82.7us); ]\n",
            "\n",
            "Epoch 68 (step 68):\n",
            "\tLearning rate: 1.76e+00\n",
            "\tTraining loss: 5.43e-04\n",
            "\tImprovement: 1.69e-04\n",
            "\tLogging to: ./logs/06862fd7da\n",
            "\tEpoch time: 156ms. [ prep data: 72.4% (113ms); batching: 0.0% (11.4us); train: 26.6% (41.5ms); ltl: 0.6% (1.01ms); lvl: 0.0% (10.5us); visualize: 0.2% (246us); terminate: 0.1% (91.6us); ]\n",
            "\n",
            "Epoch 69 (step 69):\n",
            "\tLearning rate: 1.76e+00\n",
            "\tTraining loss: 5.62e-04\n",
            "\tImprovement: -1.84e-05\n",
            "\tLogging to: ./logs/06862fd7da\n",
            "\tEpoch time: 137ms. [ prep data: 69.8% (95.9ms); batching: 0.0% (11.9us); train: 28.3% (38.9ms); ltl: 0.8% (1.08ms); lvl: 0.0% (10.7us); visualize: 0.2% (260us); terminate: 0.1% (87.3us); ]\n",
            "\n",
            "Epoch 70 (step 70):\n",
            "\tLearning rate: 1.76e+00\n",
            "\tTraining loss: 5.50e-04\n",
            "\tImprovement: 1.13e-05\n",
            "\tLogging to: ./logs/06862fd7da\n",
            "\tEpoch time: 127ms. [ prep data: 64.5% (81.7ms); batching: 0.0% (12.2us); train: 32.9% (41.7ms); ltl: 1.1% (1.38ms); lvl: 0.0% (9.78us); visualize: 0.3% (331us); terminate: 0.1% (134us); ]\n",
            "\n",
            "Epoch 71 (step 71):\n",
            "\tLearning rate: 1.76e+00\n",
            "\tTraining loss: 5.18e-04\n",
            "\tImprovement: 3.20e-05\n",
            "\tLogging to: ./logs/06862fd7da\n",
            "\tEpoch time: 132ms. [ prep data: 61.8% (81.6ms); batching: 0.0% (11.4us); train: 36.3% (47.9ms); ltl: 0.7% (955us); lvl: 0.0% (10.3us); visualize: 0.2% (215us); terminate: 0.1% (92.7us); ]\n",
            "\n",
            "Epoch 72 (step 72):\n",
            "\tLearning rate: 1.76e+00\n",
            "\tTraining loss: 4.58e-04\n",
            "\tImprovement: 6.05e-05\n",
            "\tLogging to: ./logs/06862fd7da\n",
            "\tEpoch time: 137ms. [ prep data: 66.3% (90.7ms); batching: 0.0% (13.1us); train: 31.5% (43.1ms); ltl: 0.9% (1.27ms); lvl: 0.0% (7.63us); visualize: 0.1% (182us); terminate: 0.1% (79.9us); ]\n",
            "\n",
            "Epoch 73 (step 73):\n",
            "\tLearning rate: 1.76e+00\n",
            "\tTraining loss: 4.56e-04\n",
            "\tImprovement: 2.07e-06\n",
            "\tLogging to: ./logs/06862fd7da\n",
            "\tEpoch time: 134ms. [ prep data: 66.3% (89.1ms); batching: 0.0% (12.9us); train: 31.9% (42.9ms); ltl: 0.7% (965us); lvl: 0.0% (10.5us); visualize: 0.2% (236us); terminate: 0.1% (86.5us); ]\n",
            "\n",
            "Epoch 74 (step 74):\n",
            "\tLearning rate: 1.76e+00\n",
            "\tTraining loss: 4.28e-04\n",
            "\tImprovement: 2.78e-05\n",
            "\tLogging to: ./logs/06862fd7da\n",
            "\tEpoch time: 150ms. [ prep data: 67.8% (102ms); batching: 0.0% (15.0us); train: 31.3% (47.1ms); ltl: 0.6% (890us); lvl: 0.0% (8.34us); visualize: 0.1% (203us); terminate: 0.1% (88.7us); ]\n",
            "\n",
            "Epoch 75 (step 75):\n",
            "\tLearning rate: 1.85e+00\n",
            "\tTraining loss: 3.58e-04\n",
            "\tImprovement: 6.97e-05\n",
            "\tLogging to: ./logs/06862fd7da\n",
            "\tEpoch time: 156ms. [ prep data: 70.9% (111ms); batching: 0.0% (13.4us); train: 27.4% (42.9ms); ltl: 0.7% (1.02ms); lvl: 0.0% (10.5us); visualize: 0.2% (283us); terminate: 0.1% (80.3us); ]\n",
            "\n",
            "Epoch 76 (step 76):\n",
            "\tLearning rate: 1.85e+00\n",
            "\tTraining loss: 3.68e-04\n",
            "\tImprovement: -9.23e-06\n",
            "\tLogging to: ./logs/06862fd7da\n",
            "\tEpoch time: 110ms. [ prep data: 63.1% (69.5ms); batching: 0.0% (16.5us); train: 34.8% (38.3ms); ltl: 0.8% (931us); lvl: 0.0% (9.78us); visualize: 0.3% (277us); terminate: 0.1% (92.7us); ]\n",
            "\n",
            "Epoch 77 (step 77):\n",
            "\tLearning rate: 1.85e+00\n",
            "\tTraining loss: 3.49e-04\n",
            "\tImprovement: 1.81e-05\n",
            "\tLogging to: ./logs/06862fd7da\n",
            "\tEpoch time: 111ms. [ prep data: 61.3% (67.8ms); batching: 0.0% (12.6us); train: 36.7% (40.6ms); ltl: 0.7% (800us); lvl: 0.0% (9.30us); visualize: 0.2% (188us); terminate: 0.1% (85.1us); ]\n",
            "\n",
            "Epoch 78 (step 78):\n",
            "\tLearning rate: 1.85e+00\n",
            "\tTraining loss: 3.33e-04\n",
            "\tImprovement: 1.67e-05\n",
            "\tLogging to: ./logs/06862fd7da\n",
            "\tEpoch time: 167ms. [ prep data: 67.0% (112ms); batching: 0.0% (11.7us); train: 31.6% (52.8ms); ltl: 0.6% (962us); lvl: 0.0% (7.15us); visualize: 0.1% (206us); terminate: 0.0% (79.9us); ]\n",
            "\n",
            "Epoch 79 (step 79):\n",
            "\tLearning rate: 1.85e+00\n",
            "\tTraining loss: 3.26e-04\n",
            "\tImprovement: 6.97e-06\n",
            "\tLogging to: ./logs/06862fd7da\n",
            "\tEpoch time: 163ms. [ prep data: 72.9% (119ms); batching: 0.0% (12.6us); train: 26.2% (42.7ms); ltl: 0.6% (1.02ms); lvl: 0.0% (10.5us); visualize: 0.1% (200us); terminate: 0.1% (84.6us); ]\n",
            "\n",
            "Epoch 80 (step 80):\n",
            "\tLearning rate: 1.85e+00\n",
            "\tTraining loss: 3.44e-04\n",
            "\tImprovement: -1.78e-05\n",
            "\tLogging to: ./logs/06862fd7da\n",
            "\tEpoch time: 122ms. [ prep data: 57.2% (69.7ms); batching: 0.0% (11.9us); train: 40.5% (49.3ms); ltl: 1.1% (1.31ms); lvl: 0.0% (10.5us); visualize: 0.2% (211us); terminate: 0.1% (85.4us); ]\n",
            "\n",
            "Epoch 81 (step 81):\n",
            "\tLearning rate: 1.85e+00\n",
            "\tTraining loss: 3.06e-04\n",
            "\tImprovement: 3.79e-05\n",
            "\tLogging to: ./logs/06862fd7da\n",
            "\tEpoch time: 131ms. [ prep data: 54.4% (71.4ms); batching: 0.0% (29.8us); train: 44.5% (58.5ms); ltl: 0.7% (901us); lvl: 0.0% (7.39us); visualize: 0.1% (172us); terminate: 0.1% (102us); ]\n",
            "\n",
            "Epoch 82 (step 82):\n",
            "\tLearning rate: 1.85e+00\n",
            "\tTraining loss: 3.05e-04\n",
            "\tImprovement: 7.84e-07\n",
            "\tLogging to: ./logs/06862fd7da\n",
            "\tEpoch time: 126ms. [ prep data: 61.8% (78.1ms); batching: 0.0% (13.6us); train: 36.0% (45.4ms); ltl: 0.8% (988us); lvl: 0.0% (9.30us); visualize: 0.2% (258us); terminate: 0.1% (84.6us); ]\n",
            "\n",
            "Epoch 83 (step 83):\n",
            "\tLearning rate: 1.85e+00\n",
            "\tTraining loss: 2.89e-04\n",
            "\tImprovement: 1.62e-05\n",
            "\tLogging to: ./logs/06862fd7da\n",
            "\tEpoch time: 141ms. [ prep data: 67.3% (95.0ms); batching: 0.3% (356us); train: 31.2% (44.0ms); ltl: 0.8% (1.11ms); lvl: 0.0% (8.11us); visualize: 0.2% (276us); terminate: 0.1% (90.6us); ]\n",
            "\n",
            "Epoch 84 (step 84):\n",
            "\tLearning rate: 1.85e+00\n",
            "\tTraining loss: 2.70e-04\n",
            "\tImprovement: 1.92e-05\n",
            "\tLogging to: ./logs/06862fd7da\n",
            "\tEpoch time: 134ms. [ prep data: 59.7% (79.9ms); batching: 0.0% (13.4us); train: 38.3% (51.3ms); ltl: 0.8% (1.08ms); lvl: 0.0% (7.63us); visualize: 0.2% (299us); terminate: 0.1% (93.5us); ]\n",
            "\n",
            "Epoch 85 (step 85):\n",
            "\tLearning rate: 1.85e+00\n",
            "\tTraining loss: 2.52e-04\n",
            "\tImprovement: 1.78e-05\n",
            "\tLogging to: ./logs/06862fd7da\n",
            "\tEpoch time: 148ms. [ prep data: 67.7% (100ms); batching: 0.0% (16.0us); train: 30.8% (45.7ms); ltl: 0.8% (1.13ms); lvl: 0.0% (8.34us); visualize: 0.2% (258us); terminate: 0.1% (91.1us); ]\n",
            "\n",
            "Epoch 86 (step 86):\n",
            "\tLearning rate: 1.95e+00\n",
            "\tTraining loss: 2.52e-04\n",
            "\tImprovement: -1.95e-07\n",
            "\tLogging to: ./logs/06862fd7da\n",
            "\tEpoch time: 131ms. [ prep data: 63.3% (82.6ms); batching: 0.0% (15.7us); train: 35.3% (46.2ms); ltl: 0.9% (1.14ms); lvl: 0.0% (9.78us); visualize: 0.3% (358us); terminate: 0.1% (90.4us); ]\n",
            "\n",
            "Epoch 87 (step 87):\n",
            "\tLearning rate: 1.95e+00\n",
            "\tTraining loss: 2.27e-04\n",
            "\tImprovement: 2.54e-05\n",
            "\tLogging to: ./logs/06862fd7da\n",
            "\tEpoch time: 151ms. [ prep data: 64.9% (97.9ms); batching: 0.0% (15.3us); train: 33.4% (50.3ms); ltl: 0.7% (1.03ms); lvl: 0.0% (7.87us); visualize: 0.2% (232us); terminate: 0.1% (82.0us); ]\n",
            "\n",
            "Epoch 88 (step 88):\n",
            "\tLearning rate: 1.95e+00\n",
            "\tTraining loss: 2.27e-04\n",
            "\tImprovement: -2.72e-08\n",
            "\tLogging to: ./logs/06862fd7da\n",
            "\tEpoch time: 151ms. [ prep data: 57.2% (86.1ms); batching: 0.0% (12.6us); train: 40.7% (61.3ms); ltl: 0.9% (1.40ms); lvl: 0.0% (7.87us); visualize: 0.1% (148us); terminate: 0.1% (85.6us); ]\n",
            "\n",
            "Epoch 89 (step 89):\n",
            "\tLearning rate: 1.95e+00\n",
            "\tTraining loss: 2.21e-04\n",
            "\tImprovement: 5.32e-06\n",
            "\tLogging to: ./logs/06862fd7da\n",
            "\tEpoch time: 161ms. [ prep data: 72.0% (116ms); batching: 0.0% (30.5us); train: 27.0% (43.5ms); ltl: 0.6% (997us); lvl: 0.0% (7.87us); visualize: 0.1% (238us); terminate: 0.1% (85.1us); ]\n",
            "\n",
            "Epoch 90 (step 90):\n",
            "\tLearning rate: 1.95e+00\n",
            "\tTraining loss: 2.04e-04\n",
            "\tImprovement: 1.75e-05\n",
            "\tLogging to: ./logs/06862fd7da\n",
            "\tEpoch time: 123ms. [ prep data: 63.6% (78.0ms); batching: 0.0% (13.4us); train: 35.2% (43.1ms); ltl: 0.9% (1.06ms); lvl: 0.0% (7.63us); visualize: 0.1% (140us); terminate: 0.1% (83.2us); ]\n",
            "\n",
            "Epoch 91 (step 91):\n",
            "\tLearning rate: 1.95e+00\n",
            "\tTraining loss: 2.00e-04\n",
            "\tImprovement: 4.03e-06\n",
            "\tLogging to: ./logs/06862fd7da\n",
            "\tEpoch time: 142ms. [ prep data: 67.8% (96.1ms); batching: 0.0% (11.4us); train: 30.6% (43.4ms); ltl: 0.6% (797us); lvl: 0.0% (10.0us); visualize: 0.1% (201us); terminate: 0.1% (87.7us); ]\n",
            "\n",
            "Epoch 92 (step 92):\n",
            "\tLearning rate: 1.95e+00\n",
            "\tTraining loss: 1.94e-04\n",
            "\tImprovement: 5.32e-06\n",
            "\tLogging to: ./logs/06862fd7da\n",
            "\tEpoch time: 113ms. [ prep data: 61.7% (69.8ms); batching: 0.0% (12.4us); train: 36.9% (41.7ms); ltl: 0.9% (1.07ms); lvl: 0.0% (8.58us); visualize: 0.2% (236us); terminate: 0.1% (95.4us); ]\n",
            "\n",
            "Epoch 93 (step 93):\n",
            "\tLearning rate: 1.95e+00\n",
            "\tTraining loss: 1.81e-04\n",
            "\tImprovement: 1.35e-05\n",
            "\tLogging to: ./logs/06862fd7da\n",
            "\tEpoch time: 141ms. [ prep data: 64.8% (91.1ms); batching: 0.0% (11.7us); train: 33.4% (46.9ms); ltl: 0.9% (1.29ms); lvl: 0.0% (8.82us); visualize: 0.2% (310us); terminate: 0.1% (90.6us); ]\n",
            "\n",
            "Epoch 94 (step 94):\n",
            "\tLearning rate: 2.05e+00\n",
            "\tTraining loss: 1.82e-04\n",
            "\tImprovement: -1.42e-06\n",
            "\tLogging to: ./logs/06862fd7da\n",
            "\tEpoch time: 141ms. [ prep data: 68.5% (96.6ms); batching: 0.0% (13.6us); train: 30.4% (43.0ms); ltl: 0.7% (994us); lvl: 0.0% (10.5us); visualize: 0.2% (221us); terminate: 0.1% (101us); ]\n",
            "\n",
            "Epoch 95 (step 95):\n",
            "\tLearning rate: 2.05e+00\n",
            "\tTraining loss: 1.70e-04\n",
            "\tImprovement: 1.29e-05\n",
            "\tLogging to: ./logs/06862fd7da\n",
            "\tEpoch time: 147ms. [ prep data: 61.7% (90.9ms); batching: 0.0% (12.9us); train: 36.8% (54.2ms); ltl: 0.9% (1.36ms); lvl: 0.0% (8.34us); visualize: 0.1% (194us); terminate: 0.1% (73.9us); ]\n",
            "\n",
            "Epoch 96 (step 96):\n",
            "\tLearning rate: 2.05e+00\n",
            "\tTraining loss: 1.69e-04\n",
            "\tImprovement: 9.60e-07\n",
            "\tLogging to: ./logs/06862fd7da\n",
            "\tEpoch time: 111ms. [ prep data: 60.6% (67.3ms); batching: 0.0% (11.9us); train: 36.1% (40.1ms); ltl: 1.3% (1.39ms); lvl: 0.0% (10.3us); visualize: 0.3% (358us); terminate: 0.1% (95.4us); ]\n",
            "\n",
            "Epoch 97 (step 97):\n",
            "\tLearning rate: 2.05e+00\n",
            "\tTraining loss: 1.67e-04\n",
            "\tImprovement: 1.85e-06\n",
            "\tLogging to: ./logs/06862fd7da\n",
            "\tEpoch time: 146ms. [ prep data: 66.5% (97.3ms); batching: 0.0% (11.7us); train: 31.8% (46.5ms); ltl: 0.7% (986us); lvl: 0.0% (10.0us); visualize: 0.1% (192us); terminate: 0.0% (68.4us); ]\n",
            "\n",
            "Epoch 98 (step 98):\n",
            "\tLearning rate: 2.05e+00\n",
            "\tTraining loss: 1.59e-04\n",
            "\tImprovement: 7.84e-06\n",
            "\tLogging to: ./logs/06862fd7da\n",
            "\tEpoch time: 111ms. [ prep data: 60.1% (66.6ms); batching: 0.0% (14.5us); train: 37.4% (41.4ms); ltl: 1.6% (1.76ms); lvl: 0.0% (7.87us); visualize: 0.2% (243us); terminate: 0.1% (88.7us); ]\n",
            "\n",
            "Epoch 99 (step 99):\n",
            "\tLearning rate: 2.05e+00\n",
            "\tTraining loss: 1.54e-04\n",
            "\tImprovement: 5.01e-06\n",
            "\tLogging to: ./logs/06862fd7da\n",
            "\tEpoch time: 134ms. [ prep data: 56.3% (75.5ms); batching: 0.0% (13.6us); train: 42.4% (56.9ms); ltl: 0.9% (1.27ms); lvl: 0.0% (9.30us); visualize: 0.2% (206us); terminate: 0.1% (85.4us); ]\n",
            "\n",
            "Epoch 100 (step 100):\n",
            "\tLearning rate: 2.16e+00\n",
            "\tSaving SESO checkpoint.\n",
            "\tAchieved lowest training loss.\n",
            "\tSaving LTL checkpoint.\n",
            "\tSaving LTL summary (train).\n",
            "\tUpdating Tensorboard images.\n",
            "\tSaving SESO visualizations.\n",
            "\tTraining loss: 1.56e-04\n",
            "\tImprovement: -1.71e-06\n",
            "\tLogging to: ./logs/06862fd7da\n",
            "\tEpoch time: 1.90s. [ prep data: 5.9% (112ms); batching: 0.0% (34.3us); train: 2.3% (42.8ms); seso: 19.7% (373ms); ltl: 19.2% (364ms); lvl: 0.0% (9.30us); visualize: 52.9% (1.00s); terminate: 0.0% (112us); ]\n",
            "\n",
            "Epoch 101 (step 101):\n",
            "\tLearning rate: 2.16e+00\n",
            "\tTraining loss: 1.47e-04\n",
            "\tImprovement: 8.10e-06\n",
            "\tLogging to: ./logs/06862fd7da\n",
            "\tEpoch time: 258ms. [ prep data: 65.7% (170ms); batching: 0.0% (19.3us); train: 32.9% (84.9ms); ltl: 0.5% (1.20ms); lvl: 0.0% (9.54us); visualize: 0.1% (266us); terminate: 0.0% (124us); ]\n",
            "\n",
            "Epoch 102 (step 102):\n",
            "\tLearning rate: 2.16e+00\n",
            "\tTraining loss: 1.48e-04\n",
            "\tImprovement: -8.87e-07\n",
            "\tLogging to: ./logs/06862fd7da\n",
            "\tEpoch time: 247ms. [ prep data: 65.0% (161ms); batching: 0.0% (16.7us); train: 33.7% (83.3ms); ltl: 0.4% (1.02ms); lvl: 0.0% (10.0us); visualize: 0.1% (220us); terminate: 0.0% (91.3us); ]\n",
            "\n",
            "Epoch 103 (step 103):\n",
            "\tLearning rate: 2.16e+00\n",
            "\tTraining loss: 1.45e-04\n",
            "\tImprovement: 3.10e-06\n",
            "\tLogging to: ./logs/06862fd7da\n",
            "\tEpoch time: 289ms. [ prep data: 73.7% (213ms); batching: 0.0% (15.0us); train: 25.2% (72.9ms); ltl: 0.4% (1.04ms); lvl: 0.0% (9.78us); visualize: 0.1% (234us); terminate: 0.0% (90.8us); ]\n",
            "\n",
            "Epoch 104 (step 104):\n",
            "\tLearning rate: 2.16e+00\n",
            "\tTraining loss: 1.41e-04\n",
            "\tImprovement: 4.69e-06\n",
            "\tLogging to: ./logs/06862fd7da\n",
            "\tEpoch time: 228ms. [ prep data: 65.6% (150ms); batching: 0.0% (14.1us); train: 33.6% (76.6ms); ltl: 0.5% (1.10ms); lvl: 0.0% (9.54us); visualize: 0.1% (262us); terminate: 0.0% (97.3us); ]\n",
            "\n",
            "Epoch 105 (step 105):\n",
            "\tLearning rate: 2.16e+00\n",
            "\tTraining loss: 1.38e-04\n",
            "\tImprovement: 2.74e-06\n",
            "\tLogging to: ./logs/06862fd7da\n",
            "\tEpoch time: 259ms. [ prep data: 66.9% (173ms); batching: 0.0% (16.7us); train: 32.5% (84.1ms); ltl: 0.4% (983us); lvl: 0.0% (9.78us); visualize: 0.1% (239us); terminate: 0.0% (93.2us); ]\n",
            "\n",
            "Epoch 106 (step 106):\n",
            "\tLearning rate: 2.16e+00\n",
            "\tTraining loss: 1.33e-04\n",
            "\tImprovement: 4.76e-06\n",
            "\tLogging to: ./logs/06862fd7da\n",
            "\tEpoch time: 264ms. [ prep data: 67.1% (177ms); batching: 0.0% (23.8us); train: 32.2% (85.0ms); ltl: 0.4% (1.15ms); lvl: 0.0% (10.0us); visualize: 0.1% (277us); terminate: 0.0% (103us); ]\n",
            "\n",
            "Epoch 107 (step 107):\n",
            "\tLearning rate: 2.16e+00\n",
            "\tTraining loss: 1.31e-04\n",
            "\tImprovement: 2.45e-06\n",
            "\tLogging to: ./logs/06862fd7da\n",
            "\tEpoch time: 290ms. [ prep data: 72.3% (210ms); batching: 0.0% (14.1us); train: 27.2% (78.8ms); ltl: 0.3% (889us); lvl: 0.0% (10.0us); visualize: 0.1% (273us); terminate: 0.0% (101us); ]\n",
            "\n",
            "Epoch 108 (step 108):\n",
            "\tLearning rate: 2.27e+00\n",
            "\tTraining loss: 1.28e-04\n",
            "\tImprovement: 2.57e-06\n",
            "\tLogging to: ./logs/06862fd7da\n",
            "\tEpoch time: 286ms. [ prep data: 70.2% (201ms); batching: 0.0% (16.9us); train: 29.1% (83.3ms); ltl: 0.4% (1.14ms); lvl: 0.0% (9.78us); visualize: 0.1% (304us); terminate: 0.0% (105us); ]\n",
            "\n",
            "Epoch 109 (step 109):\n",
            "\tLearning rate: 2.27e+00\n",
            "\tTraining loss: 1.28e-04\n",
            "\tImprovement: 2.74e-07\n",
            "\tLogging to: ./logs/06862fd7da\n",
            "\tEpoch time: 239ms. [ prep data: 68.3% (163ms); batching: 0.0% (15.3us); train: 30.9% (74.1ms); ltl: 0.5% (1.25ms); lvl: 0.0% (10.5us); visualize: 0.1% (290us); terminate: 0.0% (111us); ]\n",
            "\n",
            "Epoch 110 (step 110):\n",
            "\tLearning rate: 2.27e+00\n",
            "\tTraining loss: 1.32e-04\n",
            "\tImprovement: -4.16e-06\n",
            "\tLogging to: ./logs/06862fd7da\n",
            "\tEpoch time: 217ms. [ prep data: 62.8% (136ms); batching: 0.0% (15.5us); train: 35.2% (76.5ms); ltl: 0.7% (1.54ms); lvl: 0.0% (12.9us); visualize: 0.2% (346us); terminate: 0.1% (121us); ]\n",
            "\n",
            "Epoch 111 (step 111):\n",
            "\tLearning rate: 2.27e+00\n",
            "\tTraining loss: 1.41e-04\n",
            "\tImprovement: -8.72e-06\n",
            "\tLogging to: ./logs/06862fd7da\n",
            "\tEpoch time: 272ms. [ prep data: 64.7% (176ms); batching: 0.0% (15.0us); train: 33.5% (91.2ms); ltl: 0.6% (1.64ms); lvl: 0.0% (11.0us); visualize: 0.1% (376us); terminate: 0.1% (226us); ]\n",
            "\n",
            "Epoch 112 (step 112):\n",
            "\tLearning rate: 2.27e+00\n",
            "\tTraining loss: 1.66e-04\n",
            "\tImprovement: -2.52e-05\n",
            "\tLogging to: ./logs/06862fd7da\n",
            "\tEpoch time: 261ms. [ prep data: 73.5% (192ms); batching: 0.0% (15.0us); train: 25.7% (67.0ms); ltl: 0.5% (1.32ms); lvl: 0.0% (11.2us); visualize: 0.1% (275us); terminate: 0.1% (168us); ]\n",
            "\n",
            "Epoch 113 (step 113):\n",
            "\tLearning rate: 2.16e+00\n",
            "\tTraining loss: 2.22e-04\n",
            "\tImprovement: -5.63e-05\n",
            "\tLogging to: ./logs/06862fd7da\n",
            "\tEpoch time: 211ms. [ prep data: 69.9% (148ms); batching: 0.0% (16.0us); train: 28.9% (61.1ms); ltl: 0.5% (1.06ms); lvl: 0.0% (10.7us); visualize: 0.1% (304us); terminate: 0.0% (94.4us); ]\n",
            "\n",
            "Epoch 114 (step 114):\n",
            "\tLearning rate: 2.16e+00\n",
            "\tTraining loss: 3.14e-04\n",
            "\tImprovement: -9.15e-05\n",
            "\tLogging to: ./logs/06862fd7da\n",
            "\tEpoch time: 197ms. [ prep data: 67.4% (133ms); batching: 0.0% (14.5us); train: 31.5% (62.1ms); ltl: 0.7% (1.43ms); lvl: 0.0% (10.5us); visualize: 0.2% (333us); terminate: 0.1% (104us); ]\n",
            "\n",
            "Epoch 115 (step 115):\n",
            "\tLearning rate: 2.16e+00\n",
            "\tTraining loss: 4.32e-04\n",
            "\tImprovement: -1.18e-04\n",
            "\tLogging to: ./logs/06862fd7da\n",
            "\tEpoch time: 278ms. [ prep data: 69.8% (194ms); batching: 0.0% (29.3us); train: 27.7% (76.9ms); ltl: 0.4% (1.24ms); lvl: 0.0% (10.5us); visualize: 0.1% (331us); terminate: 0.0% (123us); ]\n",
            "\n",
            "Epoch 116 (step 116):\n",
            "\tLearning rate: 2.16e+00\n",
            "\tTraining loss: 5.81e-04\n",
            "\tImprovement: -1.49e-04\n",
            "\tLogging to: ./logs/06862fd7da\n",
            "\tEpoch time: 230ms. [ prep data: 69.1% (159ms); batching: 0.0% (17.2us); train: 30.1% (69.1ms); ltl: 0.5% (1.15ms); lvl: 0.0% (9.30us); visualize: 0.1% (268us); terminate: 0.0% (100us); ]\n",
            "\n",
            "Epoch 117 (step 117):\n",
            "\tLearning rate: 2.16e+00\n",
            "\tTraining loss: 7.73e-04\n",
            "\tImprovement: -1.92e-04\n",
            "\tLogging to: ./logs/06862fd7da\n",
            "\tEpoch time: 235ms. [ prep data: 64.6% (152ms); batching: 0.0% (15.5us); train: 34.1% (80.2ms); ltl: 0.5% (1.29ms); lvl: 0.0% (11.4us); visualize: 0.2% (463us); terminate: 0.0% (106us); ]\n",
            "\n",
            "Epoch 118 (step 118):\n",
            "\tLearning rate: 2.05e+00\n",
            "\tTraining loss: 1.00e-03\n",
            "\tImprovement: -2.28e-04\n",
            "\tLogging to: ./logs/06862fd7da\n",
            "\tEpoch time: 254ms. [ prep data: 66.4% (169ms); batching: 0.0% (16.7us); train: 32.1% (81.7ms); ltl: 0.5% (1.25ms); lvl: 0.0% (9.54us); visualize: 0.1% (317us); terminate: 0.0% (98.7us); ]\n",
            "\n",
            "Epoch 119 (step 119):\n",
            "\tLearning rate: 2.05e+00\n",
            "\tTraining loss: 1.05e-03\n",
            "\tImprovement: -4.41e-05\n",
            "\tLogging to: ./logs/06862fd7da\n",
            "\tEpoch time: 221ms. [ prep data: 62.6% (139ms); batching: 0.0% (14.3us); train: 35.7% (79.0ms); ltl: 0.6% (1.26ms); lvl: 0.0% (10.5us); visualize: 0.1% (288us); terminate: 0.0% (98.2us); ]\n",
            "\n",
            "Epoch 120 (step 120):\n",
            "\tLearning rate: 2.05e+00\n",
            "\tTraining loss: 7.50e-04\n",
            "\tImprovement: 2.95e-04\n",
            "\tLogging to: ./logs/06862fd7da\n",
            "\tEpoch time: 229ms. [ prep data: 70.9% (162ms); batching: 0.0% (15.0us); train: 28.4% (65.1ms); ltl: 0.4% (1.00ms); lvl: 0.0% (10.3us); visualize: 0.1% (262us); terminate: 0.0% (103us); ]\n",
            "\n",
            "Epoch 121 (step 121):\n",
            "\tLearning rate: 2.05e+00\n",
            "\tTraining loss: 3.46e-04\n",
            "\tImprovement: 4.04e-04\n",
            "\tLogging to: ./logs/06862fd7da\n",
            "\tEpoch time: 205ms. [ prep data: 60.9% (125ms); batching: 0.0% (16.7us); train: 36.9% (75.5ms); ltl: 0.9% (1.86ms); lvl: 0.0% (10.0us); visualize: 0.2% (364us); terminate: 0.1% (145us); ]\n",
            "\n",
            "Epoch 122 (step 122):\n",
            "\tLearning rate: 2.05e+00\n",
            "\tTraining loss: 1.15e-04\n",
            "\tImprovement: 2.31e-04\n",
            "\tLogging to: ./logs/06862fd7da\n",
            "\tEpoch time: 190ms. [ prep data: 67.7% (129ms); batching: 0.0% (14.1us); train: 31.4% (59.7ms); ltl: 0.6% (1.22ms); lvl: 0.0% (10.0us); visualize: 0.2% (293us); terminate: 0.0% (93.9us); ]\n",
            "\n",
            "Epoch 123 (step 123):\n",
            "\tLearning rate: 2.05e+00\n",
            "\tTraining loss: 1.72e-04\n",
            "\tImprovement: -5.75e-05\n",
            "\tLogging to: ./logs/06862fd7da\n",
            "\tEpoch time: 242ms. [ prep data: 60.4% (146ms); batching: 0.0% (16.0us); train: 37.9% (91.6ms); ltl: 0.6% (1.38ms); lvl: 0.0% (10.3us); visualize: 0.2% (384us); terminate: 0.0% (107us); ]\n",
            "\n",
            "Epoch 124 (step 124):\n",
            "\tLearning rate: 2.05e+00\n",
            "\tTraining loss: 4.03e-04\n",
            "\tImprovement: -2.30e-04\n",
            "\tLogging to: ./logs/06862fd7da\n",
            "\tEpoch time: 249ms. [ prep data: 66.6% (166ms); batching: 0.0% (16.0us); train: 30.7% (76.4ms); ltl: 0.5% (1.37ms); lvl: 0.0% (10.0us); visualize: 0.1% (312us); terminate: 0.0% (97.8us); ]\n",
            "\n",
            "Epoch 125 (step 125):\n",
            "\tLearning rate: 2.05e+00\n",
            "\tTraining loss: 6.07e-04\n",
            "\tImprovement: -2.04e-04\n",
            "\tLogging to: ./logs/06862fd7da\n",
            "\tEpoch time: 230ms. [ prep data: 69.6% (160ms); batching: 0.0% (14.5us); train: 29.6% (68.0ms); ltl: 0.5% (1.19ms); lvl: 0.0% (9.78us); visualize: 0.1% (274us); terminate: 0.1% (144us); ]\n",
            "\n",
            "Epoch 126 (step 126):\n",
            "\tLearning rate: 2.05e+00\n",
            "\tTraining loss: 6.44e-04\n",
            "\tImprovement: -3.69e-05\n",
            "\tLogging to: ./logs/06862fd7da\n",
            "\tEpoch time: 169ms. [ prep data: 76.1% (129ms); batching: 0.0% (12.6us); train: 23.1% (39.1ms); ltl: 0.5% (849us); lvl: 0.0% (7.63us); visualize: 0.1% (198us); terminate: 0.0% (73.2us); ]\n",
            "\n",
            "Epoch 127 (step 127):\n",
            "\tLearning rate: 1.95e+00\n",
            "\tTraining loss: 5.02e-04\n",
            "\tImprovement: 1.42e-04\n",
            "\tLogging to: ./logs/06862fd7da\n",
            "\tEpoch time: 171ms. [ prep data: 71.0% (121ms); batching: 0.0% (15.0us); train: 27.7% (47.3ms); ltl: 1.0% (1.70ms); lvl: 0.0% (10.5us); visualize: 0.2% (280us); terminate: 0.1% (94.4us); ]\n",
            "\n",
            "Epoch 128 (step 128):\n",
            "\tLearning rate: 1.95e+00\n",
            "\tTraining loss: 2.40e-04\n",
            "\tImprovement: 2.62e-04\n",
            "\tLogging to: ./logs/06862fd7da\n",
            "\tEpoch time: 128ms. [ prep data: 66.0% (84.2ms); batching: 0.0% (11.4us); train: 32.9% (41.9ms); ltl: 0.7% (889us); lvl: 0.0% (10.3us); visualize: 0.2% (204us); terminate: 0.1% (112us); ]\n",
            "\n",
            "Epoch 129 (step 129):\n",
            "\tLearning rate: 1.95e+00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/training/saver.py:1052: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Stopping optimization: loss meets convergence criteria.\n",
            "\tTraining loss: 9.56e-05\n",
            "\tImprovement: 1.44e-04\n",
            "\tLogging to: ./logs/06862fd7da\n",
            "\tEpoch time: 188ms. [ prep data: 73.8% (139ms); batching: 0.0% (11.2us); train: 25.0% (47.0ms); ltl: 0.3% (570us); lvl: 0.0% (8.11us); visualize: 0.1% (118us); terminate: 0.4% (824us); ]\n",
            "\n",
            "\tSaving SESO checkpoint.\n",
            "\tSaving .done file.\n",
            "\n",
            "Closing training:\n",
            "\tUpdating Tensorboard images.\n",
            "\tSaving SESO visualizations.\n",
            "\tSaving LVL summary (train).\n",
            "\tSaving LVL predictions (train).\n",
            "\tUpdating Tensorboard images.\n",
            "\tSaving LVL visualizations.\n",
            "\tSaving LTL summary (train).\n",
            "\tSaving LTL predictions (train).\n",
            "\tUpdating Tensorboard images.\n",
            "\tSaving LTL visualizations.\n",
            "\n",
            "Total run time time: 36.8s. \n",
            "\t0.0% (986us): setup_hps\n",
            "\t0.0% (4.21ms): _setup_run_dir\n",
            "\t0.0% (1.15ms): set_random_seed\n",
            "\t0.0% (172us): init AdaptiveLearningRate\n",
            "\t0.0% (15.5us): init AdaptiveGradNormClip\n",
            "\t0.2% (62.8ms): _setup_records\n",
            "\t3.1% (1.15s): _setup_model\n",
            "\t4.7% (1.73s): _setup_optimizer\n",
            "\t0.0% (77.0us): _setup_visualizations\n",
            "\t0.5% (177ms): _setup_tensorboard\n",
            "\t0.2% (81.5ms): _setup_savers\n",
            "\t0.0% (1.51ms): _setup_session\n",
            "\t0.7% (254ms): initialize_or_restore\n",
            "\t0.5% (193ms): _setup_training\n",
            "\t3.8% (1.39s): init ckpt\n",
            "\t80.3% (29.5s): train\n",
            "\t6.0% (2.22s): close_training\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "r0DhXVms7Qka"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 2: Find, analyze, and visualize the fixed points of the trained RNN\n",
        "find_fixed_points(model, valid_predictions)"
      ],
      "metadata": {
        "id": "Bs09nElN3SCX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3de30674-87c7-4dd9-acef-bc2d9aeaac32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Searching for fixed points from 1024 initial states.\n",
            "\n",
            "\tFinding fixed points via joint optimization.\n",
            "\tOptimization complete to desired tolerance.\n",
            "\t\t526 iters\n",
            "\t\tq = 1.37e-14 +/- 2.34e-14\n",
            "\t\tdq = 1.58e-14 +/- 2.79e-13\n",
            "\t\tlearning rate = 1.96e+01\n",
            "\t\tavg iter time = 3.27e-03 sec\n",
            "\tIdentified 27 unique fixed points.\n",
            "\t\tinitial_states: 0 outliers detected (of 1024).\n",
            "\t\tfixed points: 0 outliers detected (of 27).\n",
            "\tComputing recurrent Jacobian at 27 unique fixed points.\n",
            "\tComputing input Jacobian at 27 unique fixed points.\n",
            "\tDecomposing Jacobians in a single batch.\n",
            "\tSorting by Eigenvalue magnitude.\n",
            "\tFixed point finding complete.\n",
            "\n"
          ]
        }
      ]
    }
  ]
}